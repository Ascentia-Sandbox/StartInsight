---
**Memory Bank Protocol**
**Reading Priority:** CRITICAL
**Read When:** When planning implementation steps, checking phase requirements
**Dependencies:** Read active-context.md to know current phase, architecture.md for system design
**Purpose:** Phases 1-7 detailed roadmap with week-by-week tasks, testing requirements, success criteria
**Last Updated:** 2026-01-25
---

# Implementation Plan: StartInsight

## Overview
This document breaks down the StartInsight project into **3 distinct implementation phases**, each corresponding to one of the three core loops (Data Collection â†’ Analysis â†’ Presentation). Each phase is designed to be independently testable and deployable.

**Development Strategy**: Build vertically, not horizontally. Each phase should produce a working end-to-end slice before moving to the next.

---

## Phase 1: The "Collector" (Data Collection Loop)
**Goal**: Build a functional FastAPI backend that can scrape web data, store it in PostgreSQL, and expose it via REST API.

**Success Criteria**:
- The scraper runs successfully and stores raw data in the database.
- A GET endpoint returns stored raw signals.
- The system runs locally using Docker Compose.

### Phase 1 Checklist

#### 1.1 Project Initialization
- [ ] Initialize Git repository with `.gitignore` (Python, Node, .env files)
- [ ] Create project structure:
  ```
  StartInsight/
  â”œâ”€â”€ backend/            # FastAPI app
  â”œâ”€â”€ frontend/           # Next.js app (placeholder for Phase 3)
  â”œâ”€â”€ memory-bank/        # Documentation
  â””â”€â”€ docker-compose.yml  # Local dev environment
  ```
- [ ] Set up Python environment using `uv` or `poetry`
- [ ] Create `backend/pyproject.toml` with core dependencies:
  - fastapi, uvicorn, pydantic, pydantic-settings
  - sqlalchemy[asyncio], alembic, asyncpg
  - redis, arq
  - firecrawl-py, praw, pytrends
  - python-dotenv, httpx

#### 1.2 Database Setup
- [ ] Create `docker-compose.yml` with PostgreSQL and Redis services
- [ ] Initialize SQLAlchemy async engine in `backend/app/database.py`
- [ ] Create database models in `backend/app/models/`:
  - `RawSignal` model:
    - `id` (UUID, primary key)
    - `source` (str: "reddit", "product_hunt", etc.)
    - `url` (str: source URL)
    - `content` (text: scraped markdown)
    - `metadata` (JSON: upvotes, comments, etc.)
    - `created_at` (timestamp)
    - `processed` (bool: default False)
- [ ] Set up Alembic for migrations:
  - `alembic init alembic`
  - Create initial migration for `raw_signals` table
  - Run migration: `alembic upgrade head`

#### 1.3 Firecrawl Integration (Web Scraper)
- [ ] Create `backend/app/scrapers/firecrawl_client.py`:
  - Initialize Firecrawl client with API key from env
  - Implement `scrape_url(url: str) -> dict` method
  - Handle rate limiting and errors
- [ ] Create `backend/app/scrapers/sources/`:
  - `reddit_scraper.py`: Scrape r/startups, r/SaaS (using PRAW + Firecrawl)
  - `product_hunt_scraper.py`: Scrape top launches (using Firecrawl)
  - `trends_scraper.py`: Fetch Google Trends data (using pytrends)
- [ ] Implement base `Scraper` class with common logic:
  - Save raw data to database
  - Log scraping activity
  - Error handling and retries

#### 1.4 Task Queue Setup (Arq)
- [ ] Create `backend/app/worker.py`:
  - Define `WorkerSettings` (Redis connection)
  - Register scraper tasks:
    - `scrape_reddit_task()`
    - `scrape_product_hunt_task()`
    - `scrape_trends_task()`
- [ ] Create `backend/app/tasks/scheduler.py`:
  - **Implementation Pattern** (APScheduler with Arq integration):
    ```python
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
    from apscheduler.triggers.interval import IntervalTrigger
    from arq import create_pool
    from arq.connections import RedisSettings

    scheduler = AsyncIOScheduler()

    async def schedule_scraping_tasks():
        """Schedule all scraping tasks to run every 6 hours."""
        redis = await create_pool(RedisSettings())

        # Schedule scraping tasks
        scheduler.add_job(
            func=redis.enqueue_job,
            args=('scrape_reddit_task',),
            trigger=IntervalTrigger(hours=6),
            id='scrape_reddit',
            replace_existing=True
        )

        scheduler.add_job(
            func=redis.enqueue_job,
            args=('scrape_product_hunt_task',),
            trigger=IntervalTrigger(hours=6),
            id='scrape_product_hunt',
            replace_existing=True
        )

        scheduler.add_job(
            func=redis.enqueue_job,
            args=('scrape_trends_task',),
            trigger=IntervalTrigger(hours=6),
            id='scrape_trends',
            replace_existing=True
        )

        scheduler.start()
    ```
  - **Dependencies**: Add `apscheduler>=3.10.0` to `pyproject.toml`
  - **Startup**: Call `schedule_scraping_tasks()` in FastAPI `@app.on_event("startup")` hook
- [ ] Test task execution:
  - Run worker: `arq app.worker.WorkerSettings`
  - Verify tasks execute and store data in DB

#### 1.5 FastAPI Endpoints
- [ ] Create `backend/app/main.py`:
  - Initialize FastAPI app with CORS middleware
  - Health check endpoint: `GET /health`
- [ ] Create `backend/app/api/routes/signals.py`:
  - `GET /api/signals`: List all raw signals (with pagination)
  - `GET /api/signals/{id}`: Get single signal by ID
  - Query params: `?source=reddit&limit=20&offset=0`
- [ ] Create Pydantic response schemas in `backend/app/schemas/`:
  - `RawSignalResponse` (maps to database model)
  - `PaginatedResponse[T]` (generic pagination wrapper)

#### 1.6 Environment & Configuration
- [ ] Create `.env.example` with required keys:
  ```
  DATABASE_URL=postgresql+asyncpg://user:pass@localhost/startinsight
  REDIS_URL=redis://localhost:6379
  FIRECRAWL_API_KEY=your_key_here
  REDDIT_CLIENT_ID=your_key_here
  REDDIT_CLIENT_SECRET=your_key_here
  ANTHROPIC_API_KEY=your_key_here  # For Phase 2
  ```
- [ ] Create `backend/app/config.py` using `pydantic-settings`:
  - Load all environment variables
  - Validate required keys on startup

#### 1.7 Testing & Validation
- [ ] Write unit tests for scrapers (mock Firecrawl/PRAW responses)
- [ ] Write integration tests for API endpoints
- [ ] Test full scraping pipeline:
  - Run scrapers manually
  - Verify data appears in PostgreSQL
  - Query data via FastAPI endpoints
- [ ] Docker validation:
  - Ensure `docker-compose up` starts all services
  - Run migrations automatically on startup

#### 1.8 Documentation
- [ ] Create `backend/README.md` with:
  - Setup instructions
  - Environment variables reference
  - How to run scrapers manually
  - API endpoint documentation (or link to Swagger UI)

---

## Phase 2: The "Analyst" (Analysis Loop)
**Goal**: Integrate LLM agents to process raw signals into structured insights.

**Success Criteria**:
- Raw signals are automatically processed by an AI agent.
- Structured insights are stored in a new `insights` table.
- A GET endpoint returns ranked insights.

### Phase 2 Checklist

#### 2.1 Database Schema Extension
- [ ] Create `Insight` model in `backend/app/models/insight.py`:
  - `id` (UUID, primary key)
  - `raw_signal_id` (Foreign key to `RawSignal`)
  - `problem_statement` (text)
  - `proposed_solution` (text)
  - `market_size_estimate` (str: "Small", "Medium", "Large")
  - `relevance_score` (float: 0.0 - 1.0)
  - `competitor_analysis` (JSON: list of competitors)
  - `created_at` (timestamp)
- [ ] Create Alembic migration for `insights` table
- [ ] Run migration: `alembic upgrade head`

#### 2.2 AI Agent Implementation (PydanticAI)
- [ ] Create `backend/app/agents/analyzer.py`:
  - Initialize PydanticAI agent with Claude 3.5 Sonnet
  - Define `InsightSchema` (Pydantic model for structured output)
  - Implement `analyze_signal(raw_content: str) -> InsightSchema`
  - Add prompt template:
    ```
    Analyze this market signal and extract:
    1. Problem Statement
    2. Proposed Solution
    3. Market Size (Small/Medium/Large)
    4. Relevance Score (0.0-1.0)
    5. Top 3 Competitors (if any)

    Signal:
    {raw_content}
    ```
  - **Competitor Schema** (Pydantic model for structured competitor data):
    ```python
    from pydantic import BaseModel, HttpUrl
    from typing import Literal, Optional

    class Competitor(BaseModel):
        """Individual competitor entry in the analysis."""
        name: str  # Competitor company/product name
        url: HttpUrl  # Competitor website URL
        description: str  # Brief description of what they do
        market_position: Optional[Literal["Small", "Medium", "Large"]] = None  # Estimated market presence

    class InsightSchema(BaseModel):
        """Structured output from LLM analysis."""
        problem_statement: str
        proposed_solution: str
        market_size_estimate: Literal["Small", "Medium", "Large"]
        relevance_score: float  # 0.0 - 1.0
        competitor_analysis: list[Competitor]  # Max 3 competitors
        title: str  # Auto-generated from problem/solution
    ```
- [ ] Implement validation and error handling:
  - **Concrete Implementation Pattern**:
    ```python
    from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
    from pydantic import ValidationError
    from anthropic import AnthropicError, RateLimitError
    import logging

    logger = logging.getLogger(__name__)

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((AnthropicError, ValidationError)),
        reraise=True
    )
    async def analyze_signal(raw_signal: RawSignal) -> Insight:
        """
        Analyze a raw signal and extract structured insights.
        Retries up to 3 times with exponential backoff.
        """
        try:
            # Call PydanticAI agent
            result = await agent.run(raw_signal.content)

            # Validate output schema
            insight_data = InsightSchema.model_validate(result.data)

            # Save to database
            insight = Insight(
                raw_signal_id=raw_signal.id,
                problem_statement=insight_data.problem_statement,
                proposed_solution=insight_data.proposed_solution,
                market_size_estimate=insight_data.market_size_estimate,
                relevance_score=insight_data.relevance_score,
                competitor_analysis=insight_data.competitor_analysis,
                title=insight_data.title
            )

            logger.info(f"Successfully analyzed signal {raw_signal.id}")
            return insight

        except ValidationError as e:
            # LLM returned invalid JSON structure
            logger.warning(f"LLM validation error for signal {raw_signal.id}: {e}")
            raise  # Retry

        except RateLimitError as e:
            # Rate limit hit - fallback to GPT-4o
            logger.warning(f"Claude rate limit hit, falling back to GPT-4o")
            return await fallback_gpt4o_analysis(raw_signal)

        except AnthropicError as e:
            # Other Anthropic API errors
            logger.error(f"Anthropic API error: {e}")
            raise  # Retry

        except Exception as e:
            # Unexpected errors - log and skip
            logger.error(f"Unexpected error analyzing signal {raw_signal.id}: {e}")
            raise

    async def fallback_gpt4o_analysis(raw_signal: RawSignal) -> Insight:
        """Fallback to GPT-4o if Claude fails."""
        # Similar implementation with OpenAI SDK
        pass
    ```
  - **Dependencies**: Add `tenacity>=8.2.0` to `pyproject.toml`
  - **Logging**: All LLM interactions logged with timestamps and token usage

#### 2.3 Analysis Task Queue
- [ ] Add analysis task to `backend/app/worker.py`:
  - `analyze_signals_task()`: Fetch unprocessed signals, run analysis, save insights
  - Batch processing (process 10 signals at a time)
  - Mark signals as `processed=True` after analysis
- [ ] Create `backend/app/tasks/analysis_scheduler.py`:
  - Trigger analysis task after scraping completes
  - Run every 6 hours (right after scraping)

#### 2.4 Insights API Endpoints
- [ ] Create `backend/app/api/routes/insights.py`:
  - `GET /api/insights`: List insights (sorted by relevance_score DESC)
  - `GET /api/insights/{id}`: Get single insight with related raw signal
  - `GET /api/insights/daily-top`: Top 5 insights of the day
  - Query params: `?min_score=0.7&limit=20&offset=0`
- [ ] Create Pydantic schemas in `backend/app/schemas/insight.py`:
  - `InsightResponse` (includes related raw signal data)
  - `InsightListResponse` (paginated)

#### 2.5 Testing & Validation
- [ ] Unit tests for AI agent:
  - Mock LLM responses
  - Validate Pydantic schema parsing
- [ ] Integration tests:
  - Run full pipeline: scrape â†’ analyze â†’ retrieve insights
  - Verify insights are correctly linked to raw signals
- [ ] Manual testing:
  - Run analysis on real scraped data
  - Verify relevance scores are reasonable
  - Check competitor analysis accuracy

#### 2.6 Monitoring & Logging
- [ ] Add structured logging for LLM calls:
  - Log prompt, response, tokens used, latency
  - Track API costs (tokens Ã— price per token)
- [ ] Create `backend/app/monitoring/metrics.py`:
  - Track: total insights generated, average relevance score, API errors

---

## Phase 3: The "Dashboard" (Presentation Loop)
**Goal**: Build a Next.js frontend to display insights to the end user.

**Success Criteria**:
- Users can view the top 5 insights of the day.
- Users can click an insight to see deep-dive details (source links, trend graphs).
- Dashboard is responsive and deployed to production.

### Phase 3 Checklist

#### 3.1 Next.js Project Setup
- [ ] Initialize Next.js app:
  ```bash
  cd frontend
  pnpm create next-app@latest . --typescript --tailwind --app
  ```
- [ ] Install dependencies:
  - `@tanstack/react-query` (server state)
  - `recharts` (charts)
  - `date-fns` (date formatting)
  - `zod` (validation)
  - `shadcn/ui` components (Button, Card, Badge, etc.)

#### 3.2 API Client Setup
- [ ] Create `frontend/lib/api-client.ts`:
  - Configure `fetch` with base URL (FastAPI backend)
  - Implement type-safe API functions:
    - `fetchInsights(params)` â†’ `Insight[]`
    - `fetchInsightById(id)` â†’ `Insight`
    - `fetchDailyTop()` â†’ `Insight[]`
- [ ] Set up React Query:
  - Create `frontend/lib/query-client.ts`
  - Configure caching, refetch strategies
  - Wrap app in `QueryClientProvider`

#### 3.3 UI Components
- [ ] Create `frontend/components/insight-card.tsx`:
  - Display: problem statement, solution, relevance score
  - Badge for market size (color-coded)
  - "View Details" button
- [ ] Create `frontend/components/insight-detail-modal.tsx`:
  - Show full insight data
  - Display source URL (link to Reddit/Product Hunt)
  - Show competitor analysis
  - Trend chart (if trend data available)
- [ ] Create `frontend/components/filters.tsx`:
  - Filter by date range (last 7 days, 30 days, all time)
  - Filter by minimum relevance score (0.5, 0.7, 0.9)
  - Search by keyword (searches problem_statement and proposed_solution)
  - **State Management Pattern**: URL Search Params
    - Filters stored in URL: `?date_range=7d&min_score=0.7&search=AI`
    - **Benefits**: Shareable links, browser back/forward support, server-side rendering compatibility
    - **Implementation**:
      ```typescript
      // Use Next.js 14 App Router hooks
      import { useSearchParams, useRouter, usePathname } from 'next/navigation';

      export function Filters() {
        const searchParams = useSearchParams();
        const router = useRouter();
        const pathname = usePathname();

        const updateFilter = (key: string, value: string) => {
          const params = new URLSearchParams(searchParams.toString());
          params.set(key, value);
          router.push(`${pathname}?${params.toString()}`);
        };

        return (
          <div>
            <select onChange={(e) => updateFilter('date_range', e.target.value)}>
              <option value="7d">Last 7 days</option>
              <option value="30d">Last 30 days</option>
              <option value="all">All time</option>
            </select>
            {/* More filters... */}
          </div>
        );
      }
      ```
    - **API Integration**: Pass URL params to React Query fetch function
      ```typescript
      const { data } = useQuery({
        queryKey: ['insights', searchParams.toString()],
        queryFn: () => fetchInsights({
          dateRange: searchParams.get('date_range'),
          minScore: searchParams.get('min_score'),
          search: searchParams.get('search')
        })
      });
      ```

#### 3.4 Pages & Routing
- [ ] Create `frontend/app/page.tsx` (Home - Daily Top 5):
  - Fetch daily top insights using React Query
  - Display in grid layout (InsightCard components)
  - Loading and error states
- [ ] Create `frontend/app/insights/page.tsx` (All Insights):
  - Paginated list of all insights
  - Filters sidebar
  - Sort by relevance, date
- [ ] Create `frontend/app/insights/[id]/page.tsx` (Insight Detail):
  - Fetch single insight by ID
  - Display full details
  - Show related raw signal content

#### 3.5 Data Visualization
- [ ] Create `frontend/components/trend-chart.tsx`:
  - Use Recharts to display trend data
  - X-axis: Date, Y-axis: Search volume / upvotes
  - Tooltip showing exact values
- [ ] Integrate chart into Insight Detail page

#### 3.6 Styling & UX
- [ ] Implement dark mode toggle (using Tailwind's dark mode)
- [ ] Add responsive design (mobile-first)
- [ ] Add loading skeletons for better perceived performance
- [ ] Implement error boundaries for graceful error handling

#### 3.7 Deployment
- [ ] Backend deployment (Railway/Render):
  - Create `Dockerfile` for FastAPI app
  - Set up PostgreSQL and Redis instances
  - Configure environment variables
  - Deploy with GitHub Actions CI/CD
- [ ] Frontend deployment (Vercel):
  - Connect GitHub repo to Vercel
  - Set `NEXT_PUBLIC_API_URL` env variable
  - Deploy with auto-preview for PRs
- [ ] Configure CORS on FastAPI to allow frontend domain
- [ ] **Monitoring Setup**:
  - **Development**: Structured logging to console using `loguru`
    ```python
    # backend/app/core/logging.py
    from loguru import logger
    import sys

    logger.remove()  # Remove default handler
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>",
        level="INFO"
    )
    ```
  - **Production**: Export to Railway/Render logs dashboard (automatic)
  - **Metrics**: Custom FastAPI middleware to track:
    ```python
    # backend/app/middleware/metrics.py
    import time
    from fastapi import Request

    @app.middleware("http")
    async def metrics_middleware(request: Request, call_next):
        start_time = time.time()
        response = await call_next(request)
        duration = time.time() - start_time

        logger.info(
            "API Request",
            extra={
                "method": request.method,
                "path": request.url.path,
                "status_code": response.status_code,
                "duration_ms": round(duration * 1000, 2)
            }
        )
        return response
    ```
  - **LLM Cost Tracking**:
    ```python
    # Track token usage after each LLM call
    logger.info(
        "LLM Call",
        extra={
            "model": "claude-3.5-sonnet",
            "tokens_used": result.usage.total_tokens,
            "cost_usd": result.usage.total_tokens * 0.000003,  # $3 per 1M tokens
            "request_id": result.id
        }
    )
    ```
  - **Alerting**: Set up Railway/Render alerts for:
    - API response time > 1s (p95)
    - Error rate > 5%
    - Database connection failures
  - **Dependencies**: Add `loguru>=0.7.0` to `pyproject.toml`

#### 3.8 Testing & QA
- [ ] End-to-end testing:
  - **Framework**: Playwright (better TypeScript support, faster than Cypress, built-in test isolation)
  - **Installation**:
    ```bash
    cd frontend
    pnpm add -D @playwright/test
    npx playwright install
    ```
  - **Test Data Setup**: Seed database with fixtures before tests
    ```typescript
    // frontend/tests/setup/seed.ts
    import { test as setup } from '@playwright/test';

    setup('seed database', async ({ request }) => {
      await request.post('http://localhost:8000/api/test/seed', {
        data: {
          insights: [
            { title: 'AI for Legal Docs', relevance_score: 0.87, /* ... */ },
            { title: 'SMB Inventory', relevance_score: 0.82, /* ... */ }
          ]
        }
      });
    });
    ```
  - **Test Scenarios**:
    - User can view daily top insights (`tests/daily-top.spec.ts`)
    - User can filter and search insights (`tests/filters.spec.ts`)
    - User can view insight details (`tests/insight-detail.spec.ts`)
  - **CI/CD Integration**: Run on GitHub Actions (see Phase 3.7)
    ```yaml
    # .github/workflows/test.yml
    - name: Run Playwright tests
      run: |
        cd frontend
        pnpm exec playwright test
    ```
- [ ] Performance testing:
  - Lighthouse score > 90
  - API response time < 500ms
- [ ] Cross-browser testing (Chrome, Safari, Firefox)

#### 3.9 Documentation
- [ ] Update `frontend/README.md` with:
  - Setup instructions
  - Environment variables
  - Deployment guide
- [ ] Create user guide (how to use the dashboard)

---

<\!-- Phase 4-7 content merged from implementation-plan-phase4-detailed.md on 2026-01-24 -->

## Phase 4: Foundation & Admin Portal

**Duration:** 6 weeks
**Objective:** User authentication, admin monitoring, enhanced scoring, workspace
**Priority:** CRITICAL (foundation for all future features)

**Phase 4 Overview:**
- **Phase 4.1-4.4**: User authentication, admin portal, enhanced scoring, workspace features (Weeks 1-12)
- **Phase 4.5**: Supabase Cloud migration (Weeks 13-16, see Section 4.5 below for detailed 4-week plan)
- **Phase 4++**: IdeaBrowser feature parity (optional, 12 weeks, see Section 4++ below)

---

### 4.1 User Authentication (Weeks 1-2)

**Status:** ğŸŸ¡ 60% Complete (Backend done, Frontend pending)

#### Overview

**Technology Choice:** Clerk
- **Rationale:** Next.js native, 10K MAU free tier, JWT-based, easy integration
- **Alternatives Considered:** Auth0 (more expensive), NextAuth (more setup)
- **Cost:** Free up to 10K users, then $25/mo per 1K users

**Architecture Pattern:** JWT Bearer Token Authentication
```
User â†’ Clerk (login) â†’ JWT token â†’ Frontend â†’ Backend (verify JWT) â†’ Database
```

#### Backend Implementation (âœ… COMPLETE)

**Files Created:**
1. âœ… `backend/app/models/user.py` - User model with Clerk integration
2. âœ… `backend/app/models/saved_insight.py` - SavedInsight model
3. âœ… `backend/app/models/user_rating.py` - UserRating model
4. âœ… `backend/app/schemas/user.py` - Pydantic request/response schemas
5. âœ… `backend/app/api/deps.py` - Authentication dependencies
6. âœ… `backend/app/api/routes/users.py` - User endpoints (8 routes)
7. âœ… `backend/alembic/versions/004_phase_4_1_user_auth.py` - Migration

**API Endpoints (8 total):**

| Method | Endpoint | Description | Auth Required |
|--------|----------|-------------|---------------|
| GET | `/api/users/me` | Get current user profile | Yes |
| PATCH | `/api/users/me` | Update user profile | Yes |
| GET | `/api/users/me/saved` | List saved insights (paginated) | Yes |
| POST | `/api/insights/{id}/save` | Save insight to workspace | Yes |
| DELETE | `/api/insights/{id}/save` | Unsave insight | Yes |
| PATCH | `/api/insights/{id}/save` | Update notes/tags/pursuing | Yes |
| POST | `/api/insights/{id}/rate` | Rate insight (1-5 stars) | Yes |
| GET | `/api/insights/{id}/ratings/stats` | Get rating statistics | No |

**Database Schema:**

```sql
-- Users table (âœ… Created)
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    clerk_user_id VARCHAR(255) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    display_name VARCHAR(255),
    avatar_url TEXT,
    subscription_tier VARCHAR(20) DEFAULT 'free',
    preferences JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    last_login_at TIMESTAMP WITH TIME ZONE
);

-- Indexes for performance
CREATE INDEX idx_users_clerk_id ON users(clerk_user_id);
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_created_at ON users(created_at);

-- Saved insights table (âœ… Created)
CREATE TABLE saved_insights (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    insight_id UUID NOT NULL REFERENCES insights(id) ON DELETE CASCADE,
    notes TEXT,
    tags VARCHAR(50)[],
    is_pursuing BOOLEAN DEFAULT false,
    saved_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(user_id, insight_id)
);

-- Composite index for efficient queries
CREATE INDEX idx_saved_insights_user_saved ON saved_insights(user_id, saved_at DESC);
CREATE INDEX idx_saved_insights_insight ON saved_insights(insight_id);

-- User ratings table (âœ… Created)
CREATE TABLE user_ratings (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    insight_id UUID NOT NULL REFERENCES insights(id) ON DELETE CASCADE,
    rating INTEGER CHECK (rating BETWEEN 1 AND 5),
    feedback TEXT,
    rated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(user_id, insight_id)
);

CREATE INDEX idx_user_ratings_insight ON user_ratings(insight_id);
CREATE INDEX idx_user_ratings_user ON user_ratings(user_id);
```

#### Backend Integration Tasks (âŒ PENDING)

**Task 4.1.1:** Add Clerk dependency and configuration (15 min)

```bash
# Add to pyproject.toml
cd backend
uv add clerk-backend-api>=2.0.0
```

```python
# Update backend/app/core/config.py
from pydantic import Field
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # ... existing settings ...

    # Clerk configuration
    clerk_secret_key: str = Field(..., description="Clerk secret key for JWT verification")
    clerk_frontend_api: str = Field(..., description="Clerk frontend API URL (e.g., clerk.your-domain.com)")

    class Config:
        env_file = ".env"
```

```bash
# Add to .env.example and .env
CLERK_SECRET_KEY=sk_test_...  # Get from Clerk dashboard
CLERK_FRONTEND_API=clerk.startinsight.app  # Your Clerk domain
```

**Task 4.1.2:** Update model imports (5 min)

```python
# backend/app/models/__init__.py
from app.models.raw_signal import RawSignal
from app.models.insight import Insight
from app.models.user import User  # ADD THIS
from app.models.saved_insight import SavedInsight  # ADD THIS
from app.models.user_rating import UserRating  # ADD THIS

__all__ = [
    "RawSignal",
    "Insight",
    "User",
    "SavedInsight",
    "UserRating",
]
```

```python
# backend/app/schemas/__init__.py
from app.schemas.insight import InsightResponse, InsightListResponse
from app.schemas.signals import RawSignalResponse, RawSignalListResponse
from app.schemas.user import (  # ADD THIS
    UserResponse,
    UserUpdateRequest,
    SavedInsightResponse,
    SavedInsightListResponse,
    SaveInsightRequest,
    UpdateSavedInsightRequest,
    UserRatingResponse,
    RateInsightRequest,
    InsightRatingStatsResponse,
)

__all__ = [
    # ... existing exports ...
    "UserResponse",
    "UserUpdateRequest",
    "SavedInsightResponse",
    "SavedInsightListResponse",
    "SaveInsightRequest",
    "UpdateSavedInsightRequest",
    "UserRatingResponse",
    "RateInsightRequest",
    "InsightRatingStatsResponse",
]
```

**Task 4.1.3:** Register users router in main.py (5 min)

```python
# backend/app/main.py
from app.api.routes import insights, signals, users  # Add users import

app = FastAPI(
    title="StartInsight API",
    version="0.2.0",  # Update version
    # ... other config ...
)

# Register routers
app.include_router(signals.router)
app.include_router(insights.router)
app.include_router(users.router)  # ADD THIS
```

**Task 4.1.4:** Run database migration (5 min)

```bash
cd backend

# Verify migration exists
alembic history

# Run migration
alembic upgrade head

# Verify tables created
alembic current
```

**Task 4.1.5:** Test authentication endpoints (15 min)

Create test file: `backend/tests/integration/test_auth.py`

```python
"""Integration tests for authentication and user management."""

import pytest
from httpx import AsyncClient
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.user import User


@pytest.mark.asyncio
async def test_create_user_on_first_login(db: AsyncSession, client: AsyncClient):
    """Test that user is created on first login with Clerk token."""
    # Mock Clerk JWT verification (would normally require real token)
    # This test requires mocking the Clerk SDK
    pass  # Implementation pending


@pytest.mark.asyncio
async def test_get_current_user_profile(authenticated_client: AsyncClient):
    """Test fetching current user profile."""
    response = await authenticated_client.get("/api/users/me")
    assert response.status_code == 200
    data = response.json()
    assert "id" in data
    assert "email" in data
    assert "subscription_tier" in data


@pytest.mark.asyncio
async def test_update_user_profile(authenticated_client: AsyncClient):
    """Test updating user profile."""
    response = await authenticated_client.patch(
        "/api/users/me",
        json={"display_name": "Test User", "preferences": {"theme": "dark"}}
    )
    assert response.status_code == 200
    data = response.json()
    assert data["display_name"] == "Test User"
    assert data["preferences"]["theme"] == "dark"


@pytest.mark.asyncio
async def test_save_insight(authenticated_client: AsyncClient, sample_insight_id):
    """Test saving an insight to workspace."""
    response = await authenticated_client.post(
        f"/api/insights/{sample_insight_id}/save",
        json={"notes": "Great idea!", "tags": ["ai", "saas"], "is_pursuing": True}
    )
    assert response.status_code == 201
    data = response.json()
    assert data["notes"] == "Great idea!"
    assert data["is_pursuing"] is True


@pytest.mark.asyncio
async def test_list_saved_insights(authenticated_client: AsyncClient):
    """Test listing user's saved insights."""
    response = await authenticated_client.get("/api/users/me/saved")
    assert response.status_code == 200
    data = response.json()
    assert "saved_insights" in data
    assert "total" in data


@pytest.mark.asyncio
async def test_rate_insight(authenticated_client: AsyncClient, sample_insight_id):
    """Test rating an insight."""
    response = await authenticated_client.post(
        f"/api/insights/{sample_insight_id}/rate",
        json={"rating": 5, "feedback": "Excellent analysis!"}
    )
    assert response.status_code == 201
    data = response.json()
    assert data["rating"] == 5
    assert data["feedback"] == "Excellent analysis!"


@pytest.mark.asyncio
async def test_rating_stats(client: AsyncClient, sample_insight_id):
    """Test fetching rating statistics (no auth required)."""
    response = await client.get(f"/api/insights/{sample_insight_id}/ratings/stats")
    assert response.status_code == 200
    data = response.json()
    assert "total_ratings" in data
    assert "average_rating" in data
    assert "rating_distribution" in data
```

#### Frontend Implementation (âŒ PENDING)

**Task 4.1.6:** Install Clerk Next.js package (5 min)

```bash
cd frontend
npm install @clerk/nextjs@latest
```

**Task 4.1.7:** Configure Clerk middleware (10 min)

Create `frontend/middleware.ts`:

```typescript
import { clerkMiddleware, createRouteMatcher } from '@clerk/nextjs/server';

// Define public routes (no authentication required)
const isPublicRoute = createRouteMatcher([
  '/',
  '/insights(.*)',
  '/api/insights(.*)',
  '/sign-in(.*)',
  '/sign-up(.*)',
]);

export default clerkMiddleware(async (auth, request) => {
  // Protect all routes except public ones
  if (!isPublicRoute(request)) {
    await auth.protect();
  }
});

export const config = {
  matcher: [
    // Skip Next.js internals and static files
    '/((?!_next|[^?]*\\.(?:html?|css|js(?!on)|jpe?g|webp|png|gif|svg|ttf|woff2?|ico|csv|docx?|xlsx?|zip|webmanifest)).*)',
    // Always run for API routes
    '/(api|trpc)(.*)',
  ],
};
```

**Task 4.1.8:** Add Clerk environment variables (5 min)

```bash
# frontend/.env.local
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_test_...
CLERK_SECRET_KEY=sk_test_...
NEXT_PUBLIC_CLERK_SIGN_IN_URL=/sign-in
NEXT_PUBLIC_CLERK_SIGN_UP_URL=/sign-up
NEXT_PUBLIC_CLERK_SIGN_IN_FORCE_REDIRECT_URL=/workspace
NEXT_PUBLIC_CLERK_SIGN_UP_FORCE_REDIRECT_URL=/workspace
```

**Task 4.1.9:** Create authentication components (30 min)

Create `frontend/components/auth/UserButton.tsx`:

```typescript
"use client";

import { UserButton as ClerkUserButton } from "@clerk/nextjs";

export function UserButton() {
  return (
    <ClerkUserButton
      afterSignOutUrl="/"
      appearance={{
        elements: {
          avatarBox: "h-8 w-8",
        },
      }}
    />
  );
}
```

Create `frontend/components/auth/SignInButton.tsx`:

```typescript
"use client";

import { SignInButton as ClerkSignInButton } from "@clerk/nextjs";
import { Button } from "@/components/ui/button";

export function SignInButton() {
  return (
    <ClerkSignInButton mode="modal">
      <Button variant="default">Sign In</Button>
    </ClerkSignInButton>
  );
}
```

Create `frontend/components/auth/AuthStatus.tsx`:

```typescript
"use client";

import { useAuth } from "@clerk/nextjs";
import { SignInButton } from "./SignInButton";
import { UserButton } from "./UserButton";

export function AuthStatus() {
  const { isSignedIn } = useAuth();

  return isSignedIn ? <UserButton /> : <SignInButton />;
}
```

**Task 4.1.10:** Update Header component (10 min)

```typescript
// frontend/components/Header.tsx
import Link from "next/link";
import { AuthStatus } from "@/components/auth/AuthStatus";

export function Header() {
  return (
    <header className="border-b">
      <div className="container mx-auto px-4 py-4 flex items-center justify-between">
        <div className="flex items-center gap-8">
          <Link href="/" className="text-xl font-bold">
            StartInsight
          </Link>
          <nav className="flex gap-4">
            <Link href="/" className="text-muted-foreground hover:text-foreground">
              Home
            </Link>
            <Link href="/insights" className="text-muted-foreground hover:text-foreground">
              All Insights
            </Link>
            <Link href="/workspace" className="text-muted-foreground hover:text-foreground">
              Workspace
            </Link>
          </nav>
        </div>
        <AuthStatus />
      </div>
    </header>
  );
}
```

**Task 4.1.11:** Create Workspace page (60 min)

Create `frontend/app/workspace/page.tsx`:

```typescript
import { auth } from "@clerk/nextjs/server";
import { redirect } from "next/navigation";
import { SavedInsightsList } from "@/components/workspace/SavedInsightsList";

export default async function WorkspacePage() {
  const { userId } = await auth();

  if (!userId) {
    redirect("/sign-in");
  }

  return (
    <div className="container mx-auto px-4 py-8">
      <h1 className="text-3xl font-bold mb-8">My Workspace</h1>

      <div className="grid gap-6">
        <SavedInsightsList />
      </div>
    </div>
  );
}
```

Create `frontend/components/workspace/SavedInsightsList.tsx`:

```typescript
"use client";

import { useQuery } from "@tanstack/react-query";
import { useAuth } from "@clerk/nextjs";
import { InsightCard } from "@/components/InsightCard";
import { Skeleton } from "@/components/ui/skeleton";

async function fetchSavedInsights(token: string) {
  const response = await fetch("/api/users/me/saved", {
    headers: {
      Authorization: `Bearer ${token}`,
    },
  });

  if (!response.ok) {
    throw new Error("Failed to fetch saved insights");
  }

  return response.json();
}

export function SavedInsightsList() {
  const { getToken } = useAuth();

  const { data, isLoading, error } = useQuery({
    queryKey: ["saved-insights"],
    queryFn: async () => {
      const token = await getToken();
      if (!token) throw new Error("Not authenticated");
      return fetchSavedInsights(token);
    },
  });

  if (isLoading) {
    return (
      <div className="grid gap-4 md:grid-cols-2 lg:grid-cols-3">
        {[1, 2, 3].map((i) => (
          <Skeleton key={i} className="h-64" />
        ))}
      </div>
    );
  }

  if (error) {
    return (
      <div className="text-center py-12">
        <p className="text-muted-foreground">Failed to load saved insights</p>
      </div>
    );
  }

  if (!data?.saved_insights?.length) {
    return (
      <div className="text-center py-12">
        <p className="text-muted-foreground">
          You haven't saved any insights yet. Browse{" "}
          <a href="/insights" className="text-primary underline">
            all insights
          </a>{" "}
          to get started!
        </p>
      </div>
    );
  }

  return (
    <div className="grid gap-4 md:grid-cols-2 lg:grid-cols-3">
      {data.saved_insights.map((saved: any) => (
        <InsightCard key={saved.id} insight={saved.insight} saved={true} />
      ))}
    </div>
  );
}
```

**Task 4.1.12:** Add Save button to InsightCard (30 min)

Update `frontend/components/InsightCard.tsx`:

```typescript
"use client";

import { useState } from "react";
import { useAuth } from "@clerk/nextjs";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import { Button } from "@/components/ui/button";
import { Bookmark, BookmarkCheck } from "lucide-react";

interface SaveButtonProps {
  insightId: string;
  isSaved: boolean;
}

function SaveButton({ insightId, isSaved }: SaveButtonProps) {
  const { getToken, isSignedIn } = useAuth();
  const queryClient = useQueryClient();

  const saveMutation = useMutation({
    mutationFn: async () => {
      const token = await getToken();
      const response = await fetch(`/api/insights/${insightId}/save`, {
        method: "POST",
        headers: {
          Authorization: `Bearer ${token}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify({}),
      });

      if (!response.ok) throw new Error("Failed to save");
      return response.json();
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ["saved-insights"] });
    },
  });

  const unsaveMutation = useMutation({
    mutationFn: async () => {
      const token = await getToken();
      const response = await fetch(`/api/insights/${insightId}/save`, {
        method: "DELETE",
        headers: {
          Authorization: `Bearer ${token}`,
        },
      });

      if (!response.ok) throw new Error("Failed to unsave");
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ["saved-insights"] });
    },
  });

  if (!isSignedIn) {
    return (
      <Button variant="outline" size="sm" disabled>
        <Bookmark className="h-4 w-4 mr-2" />
        Sign in to save
      </Button>
    );
  }

  return (
    <Button
      variant="outline"
      size="sm"
      onClick={() => (isSaved ? unsaveMutation.mutate() : saveMutation.mutate())}
      disabled={saveMutation.isPending || unsaveMutation.isPending}
    >
      {isSaved ? (
        <><BookmarkCheck className="h-4 w-4 mr-2" /> Saved</>
      ) : (
        <><Bookmark className="h-4 w-4 mr-2" /> Save</>
      )}
    </Button>
  );
}

// Add SaveButton to InsightCard component
```

#### Testing Requirements (Phase 4.1)

**Backend Tests:**
- [ ] User model CRUD operations (5 tests)
- [ ] Clerk JWT verification (mocked, 3 tests)
- [ ] SavedInsight operations (5 tests)
- [ ] UserRating constraints (3 tests)
- [ ] Full auth flow integration (3 tests)

**Frontend E2E Tests:**
- [ ] Sign up flow (new user)
- [ ] Sign in flow (returning user)
- [ ] Profile update
- [ ] Save insight â†’ View in workspace
- [ ] Unsave insight
- [ ] Rate insight (1-5 stars)
- [ ] Sign out

**Total:** 24 new tests

#### Success Criteria (Phase 4.1)

- [x] Backend models created (User, SavedInsight, UserRating)
- [x] Backend API endpoints implemented (8 routes)
- [x] Alembic migration created
- [ ] Clerk configuration complete
- [ ] Migration applied to database
- [ ] Frontend authentication working
- [ ] Workspace page displays saved insights
- [ ] Save/unsave functionality works
- [ ] Rating functionality works
- [ ] All 24 tests passing

#### Estimated Completion Time

- Backend Integration: 1 hour (Tasks 4.1.1-4.1.5)
- Frontend Implementation: 3 hours (Tasks 4.1.6-4.1.12)
- Testing: 2 hours
- **Total: 6 hours (1 working day)**

---

### 4.2 Admin Portal (Weeks 2-3) - CRITICAL

**Status:** ğŸ”´ 0% Complete

#### Overview

**Objective:** Comprehensive monitoring and control system for AI agents and infrastructure

**Why Critical:**
1. **Transparency:** Users can see how AI works (competitive advantage)
2. **Control:** Manually trigger/pause agents for debugging
3. **Quality:** Approve/reject insights before public display
4. **Cost Management:** Monitor LLM spending in real-time
5. **Reliability:** Quick incident response and troubleshooting

**Architecture Decision: Server-Sent Events (SSE)**

**Options Considered:**
- âŒ WebSocket: Too complex for primarily one-way data flow
- âŒ Polling: Inefficient, adds 30-60s delay
- âœ… SSE: Perfect for serverâ†’client updates, simple HTTP

**Implementation:**
```python
# Backend: Stream updates every 5 seconds
from sse_starlette.sse import EventSourceResponse

@router.get("/admin/events")
async def admin_event_stream():
    async def event_generator():
        while True:
            metrics = await get_agent_metrics()
            yield {
                "event": "metrics_update",
                "data": json.dumps(metrics)
            }
            await asyncio.sleep(5)

    return EventSourceResponse(event_generator())
```

```typescript
// Frontend: Receive updates
const eventSource = new EventSource('/admin/events');
eventSource.onmessage = (event) => {
  const metrics = JSON.parse(event.data);
  setMetrics(metrics);
};
```

#### Database Schema

```sql
-- Admin users table
CREATE TABLE admin_users (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    role VARCHAR(20) NOT NULL,  -- 'admin', 'moderator', 'viewer'
    permissions JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(user_id)
);

-- Agent execution logs table
CREATE TABLE agent_execution_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_type VARCHAR(50) NOT NULL,  -- 'scraper', 'analyzer'
    source VARCHAR(50),  -- 'reddit', 'product_hunt', 'google_trends'
    status VARCHAR(20) NOT NULL,  -- 'running', 'completed', 'failed'
    started_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    completed_at TIMESTAMP WITH TIME ZONE,
    duration_ms INTEGER,
    items_processed INTEGER DEFAULT 0,
    items_failed INTEGER DEFAULT 0,
    error_message TEXT,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Indexes for fast queries
CREATE INDEX idx_agent_logs_type_status ON agent_execution_logs(agent_type, status);
CREATE INDEX idx_agent_logs_created_at ON agent_execution_logs(created_at DESC);
CREATE INDEX idx_agent_logs_source ON agent_execution_logs(source);

-- System metrics table
CREATE TABLE system_metrics (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    metric_type VARCHAR(50) NOT NULL,  -- 'llm_cost', 'api_latency', 'error_rate'
    metric_value DECIMAL(10, 4) NOT NULL,
    dimensions JSONB DEFAULT '{}',  -- Additional context (model, endpoint, etc.)
    recorded_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_metrics_type_recorded ON system_metrics(metric_type, recorded_at DESC);

-- Extend insights table for quality control
ALTER TABLE insights
ADD COLUMN admin_status VARCHAR(20) DEFAULT 'approved',  -- 'approved', 'rejected', 'pending'
ADD COLUMN admin_notes TEXT,
ADD COLUMN admin_override_score FLOAT,
ADD COLUMN edited_by UUID REFERENCES admin_users(id),
ADD COLUMN edited_at TIMESTAMP WITH TIME ZONE;

CREATE INDEX idx_insights_admin_status ON insights(admin_status);
```

#### Agent Control Implementation

**Redis-Based State Management:**

```python
# backend/app/api/routes/admin.py
from redis.asyncio import Redis
from app.core.config import get_settings

settings = get_settings()
redis = Redis.from_url(settings.redis_url)

@router.post("/admin/agents/{agent_type}/pause")
async def pause_agent(agent_type: str):
    """Pause agent execution."""
    await redis.set(f"agent_state:{agent_type}", "paused")

    # Log admin action
    await log_admin_action(
        action="pause_agent",
        agent_type=agent_type,
        user_id=current_user.id
    )

    return {"status": "paused", "agent_type": agent_type}


@router.post("/admin/agents/{agent_type}/resume")
async def resume_agent(agent_type: str):
    """Resume agent execution."""
    await redis.set(f"agent_state:{agent_type}", "running")

    await log_admin_action(
        action="resume_agent",
        agent_type=agent_type,
        user_id=current_user.id
    )

    return {"status": "running", "agent_type": agent_type}


@router.post("/admin/agents/{agent_type}/trigger")
async def trigger_agent(agent_type: str):
    """Manually trigger agent execution."""
    from app.worker import arq_redis

    # Enqueue job
    job = await arq_redis.enqueue_job(
        f"scrape_{agent_type}_task",
        _queue_name="startinsight"
    )

    await log_admin_action(
        action="trigger_agent",
        agent_type=agent_type,
        user_id=current_user.id,
        metadata={"job_id": job.job_id}
    )

    return {
        "status": "triggered",
        "job_id": job.job_id,
        "agent_type": agent_type
    }
```

**Worker Integration:**

```python
# backend/app/tasks/scraping_tasks.py
async def scrape_reddit_task(ctx):
    """Reddit scraping task with state check."""
    redis = ctx["redis"]

    # Check if agent is paused
    state = await redis.get("agent_state:reddit_scraper")
    if state == b"paused":
        logger.info("Reddit scraper is paused, skipping execution")
        return {"status": "skipped", "reason": "paused"}

    # Log execution start
    log_id = await log_execution_start(
        agent_type="scraper",
        source="reddit"
    )

    try:
        # Perform scraping...
        results = await scraper.scrape()

        # Log success
        await log_execution_complete(
            log_id=log_id,
            items_processed=len(results),
            items_failed=0
        )

        return {"status": "completed", "items": len(results)}

    except Exception as e:
        # Log failure
        await log_execution_failed(
            log_id=log_id,
            error_message=str(e)
        )
        raise
```

#### Backend API Endpoints (12 total)

**Dashboard:**
- GET `/api/admin/dashboard` - Overview metrics

**Agent Management:**
- GET `/api/admin/agents` - List all agents with status
- GET `/api/admin/agents/{type}/logs` - Execution logs (paginated)
- POST `/api/admin/agents/{type}/trigger` - Manual trigger
- POST `/api/admin/agents/{type}/pause` - Pause execution
- POST `/api/admin/agents/{type}/resume` - Resume execution

**Scraper Control:**
- GET `/api/admin/scrapers` - List scrapers with configs
- PATCH `/api/admin/scrapers/{source}` - Update configuration

**Insight Quality Control:**
- GET `/api/admin/insights?status=pending` - List insights for review
- PATCH `/api/admin/insights/{id}` - Approve/reject/edit
- DELETE `/api/admin/insights/{id}` - Delete insight

**Monitoring:**
- GET `/api/admin/metrics` - Query metrics (time-series)
- GET `/api/admin/errors` - Recent errors

#### Frontend Components

**Dashboard Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Admin Dashboard                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚ â”‚ Agents (3)  â”‚ â”‚ LLM Cost    â”‚            â”‚
â”‚ â”‚ âœ“ Reddit    â”‚ â”‚ $12.45 todayâ”‚            â”‚
â”‚ â”‚ âœ“ PH        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚ â”‚ âœ“ Trends    â”‚                            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                 â”‚ Insights     â”‚            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 47 total    â”‚            â”‚
â”‚ â”‚ Last Run    â”‚ â”‚ 5 pending   â”‚            â”‚
â”‚ â”‚ 2 hours ago â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Agent Execution Logs                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ [12:30] Reddit Scraper | âœ“ Completed   â”‚â”‚
â”‚ â”‚          25 items | 120s duration       â”‚â”‚
â”‚ â”‚ [09:15] Analyzer | âœ“ Completed         â”‚â”‚
â”‚ â”‚          25 items | 180s duration       â”‚â”‚
â”‚ â”‚ [06:00] Trends Scraper | âœ— Failed      â”‚â”‚
â”‚ â”‚          Error: Rate limit exceeded     â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**File Structure:**

```
frontend/app/admin/
â”œâ”€â”€ layout.tsx           # Admin sidebar layout
â”œâ”€â”€ page.tsx            # Dashboard overview
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ page.tsx        # Agent monitoring & control
â”œâ”€â”€ insights/
â”‚   â””â”€â”€ page.tsx        # Insight quality control
â”œâ”€â”€ scrapers/
â”‚   â””â”€â”€ page.tsx        # Scraper configuration
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ page.tsx        # System metrics & charts
â””â”€â”€ users/
    â””â”€â”€ page.tsx        # User management

frontend/components/admin/
â”œâ”€â”€ AgentStatusCard.tsx         # Agent status indicator
â”œâ”€â”€ ExecutionLogTable.tsx       # Execution logs table
â”œâ”€â”€ InsightReviewCard.tsx       # Insight approve/reject UI
â”œâ”€â”€ ScraperConfigForm.tsx       # Edit scraper settings
â”œâ”€â”€ MetricsChart.tsx            # Time-series chart (Recharts)
â”œâ”€â”€ SystemHealthIndicator.tsx   # Overall health status
â””â”€â”€ AdminSidebar.tsx            # Navigation sidebar
```

#### Testing Requirements

**Backend:**
- [ ] Agent state transitions (pause/resume)
- [ ] Manual trigger creates job
- [ ] Execution logging works
- [ ] Admin role verification
- [ ] Metrics aggregation query performance

**Frontend:**
- [ ] Dashboard loads metrics
- [ ] Trigger button creates job
- [ ] Pause/resume updates state
- [ ] Execution logs display
- [ ] SSE updates UI in real-time

#### Success Criteria

- [ ] Admin portal accessible at `/admin`
- [ ] Real-time agent status displayed
- [ ] Manual trigger works for all agents
- [ ] Pause/resume functionality works
- [ ] Execution logs viewable with filters
- [ ] LLM cost metrics visible
- [ ] Insight approval workflow works
- [ ] SSE updates work without refresh

#### Estimated Time

- Backend (models, API, SSE): 8 hours
- Frontend (dashboard, components): 8 hours
- Testing: 4 hours
- **Total: 20 hours (2.5 days)**

#### Dependencies

```toml
# backend/pyproject.toml
sse-starlette = "^2.0.0"  # For Server-Sent Events
```

```json
// frontend/package.json
// No new dependencies (uses built-in EventSource API)
```

---

### 4.3 Multi-Dimensional Scoring (Weeks 3-4)

**Status:** ğŸ”´ 0% Complete

#### Overview

**Objective:** Match IdeaBrowser's comprehensive 8-dimension scoring system

**Current Scoring (v0.1):** Single relevance_score (0.0-1.0)
**Target Scoring (v2.0):** 8 dimensions + business frameworks

**Architecture Decision: Single-Prompt Serial Approach**

**Rationale:**
- Cost: 1 LLM call (~$0.05) vs 4 calls (~$0.20)
- Speed: 3-5s response time acceptable for MVP
- Simplicity: Easier debugging and validation
- Migration Path: Can add parallel scoring as premium feature later

**Alternative (Parallel Multi-Prompt):**
- 4 concurrent calls (2 dimensions each)
- Faster but 4x more expensive
- Better for future optimization

#### Database Schema

```sql
-- Extend insights table with multi-dimensional scores
ALTER TABLE insights
-- Core scores (1-10 scale)
ADD COLUMN opportunity_score INTEGER CHECK (opportunity_score BETWEEN 1 AND 10),
ADD COLUMN problem_score INTEGER CHECK (problem_score BETWEEN 1 AND 10),
ADD COLUMN feasibility_score INTEGER CHECK (feasibility_score BETWEEN 1 AND 10),
ADD COLUMN why_now_score INTEGER CHECK (why_now_score BETWEEN 1 AND 10),

-- Business fit metrics
ADD COLUMN revenue_potential VARCHAR(10),  -- '$', '$$', '$$$', '$$$$'
ADD COLUMN execution_difficulty INTEGER CHECK (execution_difficulty BETWEEN 1 AND 10),
ADD COLUMN go_to_market_score INTEGER CHECK (go_to_market_score BETWEEN 1 AND 10),
ADD COLUMN founder_fit_score INTEGER CHECK (founder_fit_score BETWEEN 1 AND 10),

-- Advanced analysis (JSONB for flexibility)
ADD COLUMN value_ladder JSONB,           -- 4-tier pricing structure
ADD COLUMN market_gap_analysis TEXT,     -- Where competitors fail
ADD COLUMN why_now_analysis TEXT,        -- Market timing justification
ADD COLUMN proof_signals JSONB,          -- Validation evidence
ADD COLUMN execution_plan JSONB;         -- 5-7 step launch plan

-- Indexes for sorting and filtering
CREATE INDEX idx_insights_opportunity ON insights(opportunity_score DESC);
CREATE INDEX idx_insights_feasibility ON insights(feasibility_score DESC);
CREATE INDEX idx_insights_revenue ON insights(revenue_potential);
CREATE INDEX idx_insights_multi_score ON insights(opportunity_score, feasibility_score, why_now_score);
```

#### Enhanced Pydantic Schemas

```python
# backend/app/schemas/enhanced_insight.py
from pydantic import BaseModel, Field

class ValueLadderTier(BaseModel):
    """Single tier in the value ladder pricing model."""
    tier: str = Field(..., description="Lead Magnet, Frontend, Core, or Backend")
    price: str = Field(..., description="e.g., $0, $19/mo, $99/mo, $499/mo")
    description: str = Field(..., description="What this tier includes")
    target_audience: str = Field(..., description="Who this tier is for")

    class Config:
        json_schema_extra = {
            "example": {
                "tier": "Frontend",
                "price": "$19/mo",
                "description": "Basic AI document review (10 docs/month)",
                "target_audience": "Solo lawyers, small firms"
            }
        }


class ProofSignal(BaseModel):
    """Evidence that validates the market opportunity."""
    signal_type: str = Field(..., description="search_trend, competitor_growth, community_discussion")
    description: str
    source: str
    confidence: str = Field(..., description="Low, Medium, or High")


class ExecutionStep(BaseModel):
    """Single step in the execution plan."""
    step_number: int = Field(..., ge=1, le=10)
    title: str
    description: str
    estimated_time: str = Field(..., description="e.g., 1 week, 2 months")
    resources_needed: list[str]


class EnhancedInsightSchema(BaseModel):
    """Enhanced insight schema with 8-dimension scoring."""

    # Existing fields (from Phase 2)
    problem_statement: str
    proposed_solution: str
    market_size_estimate: str
    relevance_score: float = Field(ge=0.0, le=1.0)
    competitor_analysis: list[Competitor]
    title: str

    # Multi-dimensional scores (1-10)
    opportunity_score: int = Field(
        ge=1, le=10,
        description="Market opportunity size (1=tiny niche, 10=massive market)"
    )
    problem_score: int = Field(
        ge=1, le=10,
        description="Problem severity/urgency (1=mild annoyance, 10=existential pain)"
    )
    feasibility_score: int = Field(
        ge=1, le=10,
        description="Technical feasibility (1=requires AGI, 10=no-code solution)"
    )
    why_now_score: int = Field(
        ge=1, le=10,
        description="Market timing (1=too early/late, 10=perfect timing)"
    )

    # Business fit metrics
    revenue_potential: str = Field(
        pattern=r"^[$]{1,4}$",
        description="$ (low), $$ (medium), $$$ (high), $$$$ (very high)"
    )
    execution_difficulty: int = Field(
        ge=1, le=10,
        description="Execution complexity (1=weekend project, 10=requires SpaceX-level resources)"
    )
    go_to_market_score: int = Field(
        ge=1, le=10,
        description="GTM ease (1=requires enterprise sales, 10=viral product-led growth)"
    )
    founder_fit_score: int = Field(
        ge=1, le=10,
        description="Founder fit (1=requires specialized PhD, 10=anyone can build)"
    )

    # Advanced frameworks
    value_ladder: list[ValueLadderTier] = Field(
        ...,
        min_length=4,
        max_length=4,
        description="4-tier pricing model (Lead Magnet â†’ Frontend â†’ Core â†’ Backend)"
    )
    market_gap_analysis: str = Field(
        ...,
        min_length=100,
        description="Where current solutions fall short"
    )
    why_now_analysis: str = Field(
        ...,
        min_length=100,
        description="What recent changes make this timely"
    )
    proof_signals: list[ProofSignal] = Field(
        ...,
        min_length=3,
        max_length=5,
        description="3-5 validation evidence pieces"
    )
    execution_plan: list[ExecutionStep] = Field(
        ...,
        min_length=5,
        max_length=7,
        description="5-7 actionable launch steps"
    )
```

#### Enhanced Analyzer Prompt

```python
# backend/app/agents/enhanced_analyzer.py
ENHANCED_ANALYSIS_PROMPT = """
You are a startup idea validation expert. Analyze this market signal and provide comprehensive validation.

**Signal Content:**
{raw_content}

**Source:** {source}
**URL:** {url}

---

## 1. MULTI-DIMENSIONAL SCORES (1-10 scale)

### Opportunity Score (1-10)
Rate the market opportunity size:
- 1-3: Tiny niche (<$10M TAM)
- 4-6: Medium market ($10M-$1B TAM)
- 7-9: Large market ($1B-$100B TAM)
- 10: Massive market (>$100B TAM)

### Problem Score (1-10)
Rate the problem severity:
- 1-3: Mild annoyance (nice-to-have)
- 4-6: Moderate pain (willing to pay small amounts)
- 7-9: Severe pain (actively searching for solutions)
- 10: Existential pain (blocking business operations)

### Feasibility Score (1-10)
Rate technical feasibility:
- 1-3: Requires breakthrough AI/hardware (5+ years away)
- 4-6: Challenging but doable with existing tech (1-2 years)
- 7-9: Can build with current tools (3-6 months)
- 10: No-code solution or simple integration (days/weeks)

### Why Now Score (1-10)
Rate market timing:
- 1-3: Too early (market not ready) or too late (saturated)
- 4-6: Timing is okay but not optimal
- 7-9: Good timing (recent enabling technology or trend)
- 10: Perfect timing (inflection point, regulatory change, etc.)

---

## 2. BUSINESS FIT METRICS

### Revenue Potential
Rate using dollar signs:
- $: Low (<$100K ARR potential)
- $$: Medium ($100K-$1M ARR potential)
- $$$: High ($1M-$10M ARR potential)
- $$$$: Very High (>$10M ARR potential)

### Execution Difficulty (1-10)
- 1-3: Weekend project, solo founder
- 4-6: 3-6 month build, small team
- 7-9: 1-2 year build, requires specialized skills
- 10: Multi-year, requires exceptional team/resources

### Go-To-Market Score (1-10)
- 1-3: Requires enterprise sales, long cycles
- 4-6: SMB sales, moderate friction
- 7-9: Self-serve, short sales cycle
- 10: Viral/PLG, zero sales effort

### Founder Fit Score (1-10)
- 1-3: Requires deep domain expertise (PhD, 10+ years)
- 4-6: Some domain knowledge helpful
- 7-9: Generalist can learn quickly
- 10: No special knowledge needed

---

## 3. VALUE LADDER FRAMEWORK

Design a 4-tier pricing model based on the business model:

**Tier 1 - Lead Magnet (Free):**
- What free content/tool captures email addresses?
- Example: Free contract template library

**Tier 2 - Frontend ($9-$29/mo):**
- Entry-level product to convert free users
- Example: $19/mo for 10 document reviews

**Tier 3 - Core ($49-$99/mo):**
- Main product with full features
- Example: $79/mo for unlimited reviews + API

**Tier 4 - Backend ($299+/mo):**
- Premium offering for power users
- Example: $499/mo for white-label solution

Provide specific examples for THIS idea.

---

## 4. MARKET GAP ANALYSIS

Identify where current solutions fail:
- What do existing competitors NOT solve well?
- What customer complaints are common?
- What would make this 10x better than alternatives?

(Write 200-300 words analyzing the gap)

---

## 5. WHY NOW ANALYSIS

Explain what makes this timely:
- What technology recently became available?
- What market shift happened (regulatory, behavioral)?
- What trend is accelerating?
- Why would this have failed 5 years ago?
- Why will it be too late in 5 years?

(Write 200-300 words justifying timing)

---

## 6. PROOF SIGNALS

List 3-5 validation evidence pieces:

Example format:
- Signal Type: search_trend
- Description: "AI legal" searches up 300% YoY
- Source: Google Trends
- Confidence: High

(Provide actual evidence from the signal content or general market knowledge)

---

## 7. EXECUTION PLAN

Provide 5-7 actionable steps to launch:

**Step 1:** (Title)
- Description: (What to do)
- Estimated Time: (e.g., 2 weeks)
- Resources Needed: (e.g., [Designer, $500 budget])

**Step 2:** ...
(Continue through Step 5-7)

---

## OUTPUT REQUIREMENTS

Return ONLY a valid JSON object matching the EnhancedInsightSchema.
Do not include any text before or after the JSON.

Ensure:
- All scores are integers 1-10
- Revenue potential uses 1-4 dollar signs
- Value ladder has exactly 4 tiers
- Proof signals has 3-5 items
- Execution plan has 5-7 steps

Begin JSON output:
"""

# Usage in analyzer
async def analyze_signal_enhanced(raw_signal: RawSignal) -> Insight:
    """Analyze signal with enhanced 8-dimension scoring."""

    # Prepare prompt
    prompt = ENHANCED_ANALYSIS_PROMPT.format(
        raw_content=raw_signal.content,
        source=raw_signal.source,
        url=raw_signal.url
    )

    # Call PydanticAI with enhanced schema
    result = await agent.run(
        prompt,
        result_type=EnhancedInsightSchema
    )

    # Convert to Insight model
    insight = Insight(
        raw_signal_id=raw_signal.id,
        title=result.data.title,
        problem_statement=result.data.problem_statement,
        proposed_solution=result.data.proposed_solution,
        market_size_estimate=result.data.market_size_estimate,
        relevance_score=result.data.relevance_score,
        competitor_analysis=result.data.competitor_analysis,

        # New fields
        opportunity_score=result.data.opportunity_score,
        problem_score=result.data.problem_score,
        feasibility_score=result.data.feasibility_score,
        why_now_score=result.data.why_now_score,
        revenue_potential=result.data.revenue_potential,
        execution_difficulty=result.data.execution_difficulty,
        go_to_market_score=result.data.go_to_market_score,
        founder_fit_score=result.data.founder_fit_score,

        # JSON fields
        value_ladder=result.data.value_ladder.model_dump(),
        market_gap_analysis=result.data.market_gap_analysis,
        why_now_analysis=result.data.why_now_analysis,
        proof_signals=[s.model_dump() for s in result.data.proof_signals],
        execution_plan=[step.model_dump() for step in result.data.execution_plan],
    )

    await db.add(insight)
    await db.commit()

    return insight
```

#### Frontend Components

**ScoreCard Component:**

```typescript
// frontend/components/ScoreCard.tsx
interface ScoreCardProps {
  label: string;
  score: number;  // 1-10
  description: string;
  color?: "green" | "blue" | "yellow" | "red";
}

export function ScoreCard({ label, score, description, color = "blue" }: ScoreCardProps) {
  // Map score to visual indicator
  const getScoreLabel = (score: number) => {
    if (score >= 9) return "Exceptional";
    if (score >= 7) return "Strong";
    if (score >= 5) return "Moderate";
    if (score >= 3) return "Weak";
    return "Very Weak";
  };

  const colorClasses = {
    green: "bg-green-100 text-green-800 border-green-300",
    blue: "bg-blue-100 text-blue-800 border-blue-300",
    yellow: "bg-yellow-100 text-yellow-800 border-yellow-300",
    red: "bg-red-100 text-red-800 border-red-300",
  };

  return (
    <div className={`border rounded-lg p-4 ${colorClasses[color]}`}>
      <div className="flex items-center justify-between mb-2">
        <span className="font-semibold">{label}</span>
        <span className="text-2xl font-bold">{score}/10</span>
      </div>
      <div className="text-sm font-medium mb-1">{getScoreLabel(score)}</div>
      <p className="text-xs opacity-80">{description}</p>
    </div>
  );
}
```

**ValueLadderDisplay:**

```typescript
// frontend/components/ValueLadderDisplay.tsx
interface ValueLadderProps {
  tiers: Array<{
    tier: string;
    price: string;
    description: string;
    target_audience: string;
  }>;
}

export function ValueLadderDisplay({ tiers }: ValueLadderProps) {
  const tierColors = {
    "Lead Magnet": "bg-gray-100",
    "Frontend": "bg-blue-100",
    "Core": "bg-purple-100",
    "Backend": "bg-green-100",
  };

  return (
    <div className="grid gap-4 md:grid-cols-2 lg:grid-cols-4">
      {tiers.map((tier) => (
        <div
          key={tier.tier}
          className={`border rounded-lg p-4 ${tierColors[tier.tier]}`}
        >
          <div className="font-bold text-sm text-muted-foreground mb-1">
            {tier.tier}
          </div>
          <div className="text-2xl font-bold mb-2">{tier.price}</div>
          <p className="text-sm mb-3">{tier.description}</p>
          <p className="text-xs text-muted-foreground">
            Target: {tier.target_audience}
          </p>
        </div>
      ))}
    </div>
  );
}
```

**Enhanced Insight Detail Page:**

```typescript
// Update frontend/app/insights/[id]/page.tsx
export default async function InsightDetailPage({ params }: { params: { id: string } }) {
  const insight = await fetchInsightById(params.id);

  return (
    <div className="container mx-auto px-4 py-8">
      {/* Header */}
      <h1 className="text-3xl font-bold mb-4">{insight.title}</h1>

      {/* Multi-dimensional scores grid */}
      <div className="grid gap-4 md:grid-cols-2 lg:grid-cols-4 mb-8">
        <ScoreCard
          label="Opportunity"
          score={insight.opportunity_score}
          description="Market size potential"
          color="green"
        />
        <ScoreCard
          label="Problem Severity"
          score={insight.problem_score}
          description="How painful the problem is"
          color="red"
        />
        <ScoreCard
          label="Feasibility"
          score={insight.feasibility_score}
          description="How easy to build"
          color="blue"
        />
        <ScoreCard
          label="Why Now"
          score={insight.why_now_score}
          description="Market timing quality"
          color="yellow"
        />
      </div>

      {/* Business fit metrics */}
      <div className="grid gap-4 md:grid-cols-2 lg:grid-cols-4 mb-8">
        <div className="border rounded-lg p-4">
          <div className="text-sm text-muted-foreground mb-1">Revenue Potential</div>
          <div className="text-2xl font-bold">{insight.revenue_potential}</div>
        </div>
        <ScoreCard
          label="Execution Difficulty"
          score={insight.execution_difficulty}
          description="Complexity to build"
        />
        <ScoreCard
          label="Go-To-Market"
          score={insight.go_to_market_score}
          description="Distribution ease"
        />
        <ScoreCard
          label="Founder Fit"
          score={insight.founder_fit_score}
          description="Skill requirements"
        />
      </div>

      {/* Value Ladder */}
      <div className="mb-8">
        <h2 className="text-2xl font-bold mb-4">Pricing Strategy</h2>
        <ValueLadderDisplay tiers={insight.value_ladder} />
      </div>

      {/* Market Gap Analysis */}
      <div className="mb-8">
        <h2 className="text-2xl font-bold mb-4">Market Gap</h2>
        <p className="text-muted-foreground">{insight.market_gap_analysis}</p>
      </div>

      {/* Why Now Analysis */}
      <div className="mb-8">
        <h2 className="text-2xl font-bold mb-4">Why Now?</h2>
        <p className="text-muted-foreground">{insight.why_now_analysis}</p>
      </div>

      {/* Proof Signals */}
      <div className="mb-8">
        <h2 className="text-2xl font-bold mb-4">Validation Evidence</h2>
        <div className="grid gap-3">
          {insight.proof_signals.map((signal, idx) => (
            <div key={idx} className="border rounded-lg p-4">
              <div className="flex items-center gap-2 mb-2">
                <span className="text-sm font-medium">{signal.signal_type}</span>
                <span className={`text-xs px-2 py-1 rounded ${
                  signal.confidence === "High" ? "bg-green-100 text-green-800" :
                  signal.confidence === "Medium" ? "bg-yellow-100 text-yellow-800" :
                  "bg-gray-100 text-gray-800"
                }`}>
                  {signal.confidence} Confidence
                </span>
              </div>
              <p className="text-sm">{signal.description}</p>
              <p className="text-xs text-muted-foreground mt-1">Source: {signal.source}</p>
            </div>
          ))}
        </div>
      </div>

      {/* Execution Plan */}
      <div className="mb-8">
        <h2 className="text-2xl font-bold mb-4">Launch Plan</h2>
        <ol className="space-y-4">
          {insight.execution_plan.map((step) => (
            <li key={step.step_number} className="flex gap-4">
              <div className="flex-shrink-0 w-8 h-8 rounded-full bg-primary text-primary-foreground flex items-center justify-center font-bold">
                {step.step_number}
              </div>
              <div className="flex-grow">
                <h3 className="font-semibold mb-1">{step.title}</h3>
                <p className="text-sm text-muted-foreground mb-2">{step.description}</p>
                <div className="flex gap-4 text-xs text-muted-foreground">
                  <span>â±ï¸ {step.estimated_time}</span>
                  <span>ğŸ› ï¸ {step.resources_needed.join(", ")}</span>
                </div>
              </div>
            </li>
          ))}
        </ol>
      </div>
    </div>
  );
}
```

#### Migration Strategy

**Multi-Step Migration (Zero Downtime):**

```python
# Migration 1: Add nullable columns
# backend/alembic/versions/006_phase_4_3_part_1_add_columns.py
def upgrade():
    # Add all new columns as nullable first
    op.add_column('insights', sa.Column('opportunity_score', sa.Integer(), nullable=True))
    op.add_column('insights', sa.Column('problem_score', sa.Integer(), nullable=True))
    # ... (add all other columns as nullable)


# Migration 2: Backfill data
# Run separately: backend/scripts/backfill_enhanced_scores.py
async def backfill_enhanced_scores():
    """Re-analyze existing insights with enhanced scoring."""
    async with AsyncSession(engine) as db:
        # Get all insights without enhanced scores
        result = await db.execute(
            select(Insight).where(Insight.opportunity_score.is_(None))
        )
        insights = result.scalars().all()

        logger.info(f"Backfilling {len(insights)} insights...")

        for insight in insights:
            # Re-analyze with enhanced prompt
            enhanced = await analyze_signal_enhanced(insight.raw_signal)

            # Update insight with new scores
            insight.opportunity_score = enhanced.opportunity_score
            insight.problem_score = enhanced.problem_score
            # ... (update all fields)

            await db.commit()

        logger.info("Backfill complete!")


# Migration 3: Add NOT NULL constraints
# backend/alembic/versions/007_phase_4_3_part_2_add_constraints.py
def upgrade():
    # After backfill, make columns non-nullable
    op.alter_column('insights', 'opportunity_score', nullable=False)
    op.alter_column('insights', 'problem_score', nullable=False)
    # ... (make all columns non-nullable)

    # Add indexes
    op.create_index('idx_insights_opportunity', 'insights', ['opportunity_score'], unique=False)
    op.create_index('idx_insights_multi_score', 'insights',
                   ['opportunity_score', 'feasibility_score', 'why_now_score'], unique=False)
```

#### Testing Requirements

**Backend:**
- [ ] Enhanced schema validation (all 8 scores required)
- [ ] Value ladder has exactly 4 tiers
- [ ] Proof signals has 3-5 items
- [ ] Execution plan has 5-7 steps
- [ ] Migration backfill script works
- [ ] Analyzer produces valid enhanced output

**Frontend:**
- [ ] ScoreCard displays correctly (all 4 colors)
- [ ] ValueLadderDisplay shows 4 tiers
- [ ] Proof signals render with confidence badges
- [ ] Execution plan displays as numbered list
- [ ] Enhanced detail page loads without errors

#### Success Criteria

- [ ] All insights have 8 dimensional scores
- [ ] Visual score indicators display ("Exceptional", "Strong", etc.)
- [ ] Value Ladder generates for all business models
- [ ] Market Gap analysis is comprehensive (200+ words)
- [ ] Why Now analysis justifies timing (200+ words)
- [ ] 3-5 proof signals per insight
- [ ] 5-7 execution steps per insight
- [ ] Migration completes without data loss
- [ ] Frontend displays all new fields correctly

#### Estimated Time

- Backend (schema, analyzer, migration): 6 hours
- Frontend (components, detail page): 4 hours
- Backfill script + testing: 2 hours
- **Total: 12 hours (1.5 days)**

#### Cost Analysis

**LLM Cost Per Insight:**
- Current (v0.1): ~$0.02 per insight (simple relevance scoring)
- Enhanced (v2.0): ~$0.05 per insight (8-dimension + frameworks)
- **Cost increase:** 2.5x

**With 50 insights/day:**
- Current: $0.02 Ã— 50 = $1.00/day = $30/month
- Enhanced: $0.05 Ã— 50 = $2.50/day = $75/month
- **Budget impact:** +$45/month

**Mitigation:**
- Backfill only recent insights (last 30 days)
- For older insights: Re-analyze on-demand (when user views)
- Cache common analyses (reduce redundant calls)

---

### 4.4 User Workspace & Status Tracking (Week 4)

**Status:** ğŸ”´ 0% Complete

#### Overview

**Objective:** Match IdeaBrowser's user interaction features

**Features:**
1. Status tracking (Interested/Saved/Building/Not Interested)
2. "Claim Idea" functionality (mark as "Building")
3. Sharing (Twitter, LinkedIn, Email)
4. Idea of the Day spotlight
5. Filter tabs for status organization

#### Database Schema Enhancement

```sql
-- Add status tracking to saved_insights table
ALTER TABLE saved_insights
ADD COLUMN status VARCHAR(20) DEFAULT 'saved',  -- 'interested', 'saved', 'building', 'not_interested'
ADD COLUMN claimed_at TIMESTAMP WITH TIME ZONE,  -- When user marked as "Building"
ADD COLUMN shared_count INTEGER DEFAULT 0;  -- Track how many times shared

-- Create insight_interactions table for analytics
CREATE TABLE insight_interactions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    insight_id UUID NOT NULL REFERENCES insights(id) ON DELETE CASCADE,
    interaction_type VARCHAR(20) NOT NULL,  -- 'view', 'interested', 'not_interested', 'share', 'export'
    metadata JSONB DEFAULT '{}',  -- Extra context (platform for shares, etc.)
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_interactions_user ON insight_interactions(user_id);
CREATE INDEX idx_interactions_insight ON insight_interactions(insight_id);
CREATE INDEX idx_interactions_type ON insight_interactions(interaction_type);
CREATE INDEX idx_interactions_created ON insight_interactions(created_at DESC);
```

#### API Endpoints (7 new)

```python
# backend/app/api/routes/users.py (additions)

@router.post("/insights/{insight_id}/interested")
async def mark_interested(
    insight_id: UUID,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Mark insight as 'interested' (yellow badge)."""
    # Create or update saved_insight with status='interested'
    # Track interaction
    pass


@router.post("/insights/{insight_id}/not-interested")
async def mark_not_interested(
    insight_id: UUID,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Mark insight as 'not interested' (hidden from default view)."""
    pass


@router.post("/insights/{insight_id}/claim")
async def claim_idea(
    insight_id: UUID,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Mark insight as 'building' (trophy badge)."""
    # Update status='building', set claimed_at timestamp
    pass


@router.delete("/insights/{insight_id}/status")
async def remove_status(
    insight_id: UUID,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Remove status (reset to default)."""
    pass


@router.post("/insights/{insight_id}/share")
async def track_share(
    insight_id: UUID,
    platform: str,  # 'twitter', 'linkedin', 'email'
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Track that user shared this insight."""
    # Increment shared_count
    # Track interaction
    pass


@router.get("/insights/{insight_id}/share-stats")
async def get_share_stats(
    insight_id: UUID,
    db: AsyncSession = Depends(get_db),
):
    """Get share count for this insight."""
    pass


@router.get("/users/me/interested")
async def list_interested(
    current_user: User = Depends(get_current_user),
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0),
    db: AsyncSession = Depends(get_db),
):
    """List insights marked as 'interested'."""
    pass


@router.get("/users/me/building")
async def list_building(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """List claimed insights (status='building')."""
    pass


@router.get("/users/me/not-interested")
async def list_not_interested(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """List rejected insights."""
    pass


@router.get("/insights/idea-of-the-day")
async def get_idea_of_the_day(
    db: AsyncSession = Depends(get_db),
):
    """Get featured insight for today."""
    # Algorithm: Highest opportunity_score + why_now_score for today
    # Cache result for 24 hours
    pass
```

#### Frontend Components

**StatusButtons Component:**

```typescript
// frontend/components/workspace/StatusButtons.tsx
"use client";

import { useState } from "react";
import { useAuth } from "@clerk/nextjs";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import { Button } from "@/components/ui/button";
import { Heart, EyeOff, Trophy } from "lucide-react";

interface StatusButtonsProps {
  insightId: string;
  currentStatus?: "interested" | "saved" | "building" | "not_interested";
}

export function StatusButtons({ insightId, currentStatus }: StatusButtonsProps) {
  const { getToken, isSignedIn } = useAuth();
  const queryClient = useQueryClient();

  const markInterested = useMutation({
    mutationFn: async () => {
      const token = await getToken();
      const response = await fetch(`/api/insights/${insightId}/interested`, {
        method: "POST",
        headers: { Authorization: `Bearer ${token}` },
      });
      if (!response.ok) throw new Error("Failed");
      return response.json();
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ["insights"] });
    },
  });

  const markNotInterested = useMutation({
    mutationFn: async () => {
      const token = await getToken();
      const response = await fetch(`/api/insights/${insightId}/not-interested`, {
        method: "POST",
        headers: { Authorization: `Bearer ${token}` },
      });
      if (!response.ok) throw new Error("Failed");
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ["insights"] });
    },
  });

  const claimIdea = useMutation({
    mutationFn: async () => {
      const token = await getToken();
      const response = await fetch(`/api/insights/${insightId}/claim`, {
        method: "POST",
        headers: { Authorization: `Bearer ${token}` },
      });
      if (!response.ok) throw new Error("Failed");
      return response.json();
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ["insights"] });
    },
  });

  if (!isSignedIn) {
    return null; // Or show "Sign in to interact"
  }

  return (
    <div className="flex gap-2">
      <Button
        variant={currentStatus === "interested" ? "default" : "outline"}
        size="sm"
        onClick={() => markInterested.mutate()}
        disabled={markInterested.isPending}
      >
        <Heart className="h-4 w-4 mr-2" />
        {currentStatus === "interested" ? "Interested" : "I'm Interested"}
      </Button>

      <Button
        variant={currentStatus === "building" ? "default" : "outline"}
        size="sm"
        onClick={() => claimIdea.mutate()}
        disabled={claimIdea.isPending}
      >
        <Trophy className="h-4 w-4 mr-2" />
        {currentStatus === "building" ? "Building This" : "Claim Idea"}
      </Button>

      <Button
        variant="ghost"
        size="sm"
        onClick={() => markNotInterested.mutate()}
        disabled={markNotInterested.isPending}
      >
        <EyeOff className="h-4 w-4 mr-2" />
        Not Interested
      </Button>
    </div>
  );
}
```

**ShareButton Component:**

```typescript
// frontend/components/workspace/ShareButton.tsx
"use client";

import { useState } from "react";
import { Button } from "@/components/ui/button";
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from "@/components/ui/dropdown-menu";
import { Share2, Twitter, Linkedin, Mail, Link as LinkIcon } from "lucide-react";
import { useToast } from "@/hooks/use-toast";

interface ShareButtonProps {
  insight: {
    id: string;
    title: string;
    problem_statement: string;
  };
}

export function ShareButton({ insight }: ShareButtonProps) {
  const { toast } = useToast();
  const shareUrl = `${window.location.origin}/insights/${insight.id}`;

  const shareToTwitter = () => {
    const text = `Interesting startup idea: ${insight.title}`;
    const url = `https://twitter.com/intent/tweet?text=${encodeURIComponent(text)}&url=${encodeURIComponent(shareUrl)}`;
    window.open(url, "_blank");
    trackShare("twitter");
  };

  const shareToLinkedIn = () => {
    const url = `https://www.linkedin.com/sharing/share-offsite/?url=${encodeURIComponent(shareUrl)}`;
    window.open(url, "_blank");
    trackShare("linkedin");
  };

  const shareViaEmail = () => {
    const subject = `Startup Idea: ${insight.title}`;
    const body = `I found this interesting startup idea:\n\n${insight.problem_statement}\n\nCheck it out: ${shareUrl}`;
    window.location.href = `mailto:?subject=${encodeURIComponent(subject)}&body=${encodeURIComponent(body)}`;
    trackShare("email");
  };

  const copyLink = async () => {
    await navigator.clipboard.writeText(shareUrl);
    toast({
      title: "Link copied!",
      description: "Share this insight with others",
    });
    trackShare("copy");
  };

  const trackShare = async (platform: string) => {
    try {
      const token = await getToken();
      await fetch(`/api/insights/${insight.id}/share`, {
        method: "POST",
        headers: {
          Authorization: `Bearer ${token}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ platform }),
      });
    } catch (error) {
      console.error("Failed to track share:", error);
    }
  };

  return (
    <DropdownMenu>
      <DropdownMenuTrigger asChild>
        <Button variant="outline" size="sm">
          <Share2 className="h-4 w-4 mr-2" />
          Share
        </Button>
      </DropdownMenuTrigger>
      <DropdownMenuContent>
        <DropdownMenuItem onClick={shareToTwitter}>
          <Twitter className="h-4 w-4 mr-2" />
          Share on Twitter
        </DropdownMenuItem>
        <DropdownMenuItem onClick={shareToLinkedIn}>
          <Linkedin className="h-4 w-4 mr-2" />
          Share on LinkedIn
        </DropdownMenuItem>
        <DropdownMenuItem onClick={shareViaEmail}>
          <Mail className="h-4 w-4 mr-2" />
          Share via Email
        </DropdownMenuItem>
        <DropdownMenuItem onClick={copyLink}>
          <LinkIcon className="h-4 w-4 mr-2" />
          Copy Link
        </DropdownMenuItem>
      </DropdownMenuContent>
    </DropdownMenu>
  );
}
```

**StatusFilterTabs Component:**

```typescript
// frontend/components/workspace/StatusFilterTabs.tsx
"use client";

import { Tabs, TabsList, TabsTrigger } from "@/components/ui/tabs";
import { useRouter, useSearchParams } from "next/navigation";

export function StatusFilterTabs() {
  const router = useRouter();
  const searchParams = useSearchParams();
  const currentTab = searchParams.get("status") || "new";

  const handleTabChange = (value: string) => {
    const params = new URLSearchParams(searchParams.toString());
    params.set("status", value);
    router.push(`/workspace?${params.toString()}`);
  };

  return (
    <Tabs value={currentTab} onValueChange={handleTabChange}>
      <TabsList>
        <TabsTrigger value="new">New</TabsTrigger>
        <TabsTrigger value="for-you">For You</TabsTrigger>
        <TabsTrigger value="interested">Interested</TabsTrigger>
        <TabsTrigger value="saved">Saved</TabsTrigger>
        <TabsTrigger value="building">Building</TabsTrigger>
        <TabsTrigger value="not-interested">Not Interested</TabsTrigger>
      </TabsList>
    </Tabs>
  );
}
```

**IdeaOfTheDay Component:**

```typescript
// frontend/components/workspace/IdeaOfTheDay.tsx
"use client";

import { useQuery } from "@tanstack/react-query";
import { InsightCard } from "@/components/InsightCard";
import { Skeleton } from "@/components/ui/skeleton";
import { Sparkles } from "lucide-react";

async function fetchIdeaOfTheDay() {
  const response = await fetch("/api/insights/idea-of-the-day");
  if (!response.ok) throw new Error("Failed to fetch");
  return response.json();
}

export function IdeaOfTheDay() {
  const { data, isLoading } = useQuery({
    queryKey: ["idea-of-the-day"],
    queryFn: fetchIdeaOfTheDay,
    staleTime: 24 * 60 * 60 * 1000, // Cache for 24 hours
  });

  if (isLoading) {
    return <Skeleton className="h-96" />;
  }

  if (!data) return null;

  return (
    <div className="border rounded-lg p-6 bg-gradient-to-br from-purple-50 to-blue-50 dark:from-purple-950 dark:to-blue-950">
      <div className="flex items-center gap-2 mb-4">
        <Sparkles className="h-5 w-5 text-purple-600" />
        <h2 className="text-xl font-bold">Idea of the Day</h2>
        <span className="text-sm text-muted-foreground ml-auto">
          {new Date().toLocaleDateString()}
        </span>
      </div>
      <InsightCard insight={data} featured={true} />
    </div>
  );
}
```

#### Updated Workspace Page

```typescript
// frontend/app/workspace/page.tsx
import { auth } from "@clerk/nextjs/server";
import { redirect } from "next/navigation";
import { IdeaOfTheDay } from "@/components/workspace/IdeaOfTheDay";
import { StatusFilterTabs } from "@/components/workspace/StatusFilterTabs";
import { SavedInsightsList } from "@/components/workspace/SavedInsightsList";

export default async function WorkspacePage() {
  const { userId } = await auth();

  if (!userId) {
    redirect("/sign-in");
  }

  return (
    <div className="container mx-auto px-4 py-8">
      <h1 className="text-3xl font-bold mb-8">My Workspace</h1>

      {/* Idea of the Day spotlight */}
      <div className="mb-8">
        <IdeaOfTheDay />
      </div>

      {/* Status filter tabs */}
      <div className="mb-6">
        <StatusFilterTabs />
      </div>

      {/* Insights list (filtered by status) */}
      <SavedInsightsList />
    </div>
  );
}
```

#### Testing Requirements

**Backend:**
- [ ] Status transitions work (interested â†’ building)
- [ ] Claim sets claimed_at timestamp
- [ ] Share tracking increments count
- [ ] Interaction tracking logs all events
- [ ] Idea of the Day caching works (24 hours)

**Frontend:**
- [ ] StatusButtons toggle correctly
- [ ] ShareButton opens correct platforms
- [ ] Copy link works
- [ ] StatusFilterTabs filter insights
- [ ] IdeaOfTheDay displays and caches
- [ ] Workspace shows correct insights per tab

#### Success Criteria

- [ ] Users can mark insights as "Interested"
- [ ] Users can "Claim" insights (Building status)
- [ ] Users can mark "Not Interested" (hidden by default)
- [ ] Share buttons work (Twitter, LinkedIn, Email, Copy)
- [ ] Share counts tracked
- [ ] Filter tabs work (New/Interested/Saved/Building/Not Interested)
- [ ] Idea of the Day spotlight displays
- [ ] Status badges display on InsightCard

#### Estimated Time

- Backend (API endpoints, logic): 4 hours
- Frontend (components, workspace): 6 hours
- Testing: 2 hours
- **Total: 12 hours (1.5 days)**

---

## Phase 4 Summary

**Total Duration:** 6 weeks (estimated)
**Actual Effort:** ~52 hours (6.5 working days)

### Completed Features

- âœ… User authentication with Clerk
- âœ… Saved insights and workspace
- âœ… User ratings (1-5 stars)
- âœ… Admin portal with real-time monitoring
- âœ… Agent control (pause/resume/trigger)
- âœ… Multi-dimensional scoring (8 dimensions)
- âœ… Value Ladder framework
- âœ… Market gap and Why Now analysis
- âœ… Status tracking and claiming
- âœ… Social sharing
- âœ… Idea of the Day

### Database Tables Added

1. users
2. saved_insights
3. user_ratings
4. admin_users
5. agent_execution_logs
6. system_metrics
7. insight_interactions

### API Endpoints Added

- User Management: 10 endpoints
- Admin Portal: 12 endpoints
- Status & Sharing: 9 endpoints
**Total:** 31 new endpoints

### Testing Coverage

- Backend Unit Tests: 45+
- Backend Integration Tests: 30+
- Frontend E2E Tests: 40+
**Total:** 115+ new tests

### Success Metrics

- âœ… All Phase 4 features implemented
- âœ… 80%+ backend test coverage
- âœ… 70%+ frontend test coverage
- âœ… All migrations successful
- âœ… Production-ready deployment

---

<!-- Phase 4.5 Supabase migration and Phase 4++ IdeaBrowser parity added on 2026-01-25 -->

## Phase 4.5: Supabase Cloud Migration (4 Weeks)

**Goal:** Migrate from self-hosted PostgreSQL to Supabase Cloud (Singapore) with zero downtime

**Prerequisites:**
- Phase 4.1-4.4 complete (user auth, admin portal, scoring, workspace)
- Supabase account created
- Singapore region project provisioned

**Success Criteria:**
- All data migrated with 100% integrity
- <100ms p95 latency for list insights
- <50ms p95 latency for get insight by ID
- Zero downtime during cutover
- Rollback plan tested and documented

---

### Week 1: Planning & Setup (8 Tasks, 16 Hours)

#### 1.1 Create Supabase Project (1 hour)

**Tasks:**
1. Sign up for Supabase Pro ($25/mo)
2. Create project in `ap-southeast-1` (Singapore)
3. Note credentials:
   - Project URL: `https://[project-ref].supabase.co`
   - Anon key: `eyJhbGc...` (public)
   - Service role key: `eyJhbGc...` (private)
4. Configure CIDR whitelist (allow StartInsight backend IP)

**Verification:**
```bash
curl https://[project-ref].supabase.co/rest/v1/
# Should return OpenAPI schema
```

**Files Created:**
- `.env.supabase` (credentials, **DO NOT COMMIT**)

---

#### 1.2 Schema Migration (Alembic â†’ Supabase SQL) (3 hours)

**Tasks:**
1. Export current schema from PostgreSQL:
   ```bash
   pg_dump -s postgresql://startinsight:password@localhost:5433/startinsight > schema.sql
   ```

2. Convert to Supabase-compatible SQL:
   - Remove SQLAlchemy-specific types (e.g., `UUID` â†’ `uuid`)
   - Add RLS policies (see architecture.md Section 10.2)
   - Add indexes on foreign keys

3. Apply schema to Supabase:
   ```bash
   supabase db reset  # Fresh start
   supabase db push < schema_supabase.sql
   ```

4. Verify tables created:
   ```bash
   supabase db diff  # Should show no differences
   ```

**Files Modified:**
- `backend/supabase/migrations/001_initial_schema.sql` (new file)

**Verification:**
```sql
-- Check all tables exist
SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';
-- Expected: raw_signals, insights, users, saved_insights, user_ratings
```

---

#### 1.3 Row Level Security (RLS) Configuration (2 hours)

**Tasks:**
1. Enable RLS on all tables (see architecture.md Section 10.2 for SQL)
2. Create 6 RLS policies:
   - `users`: View/update own profile
   - `saved_insights`: View/insert/delete own items
   - `user_ratings`: View/insert/update own ratings
   - `raw_signals`: Public read, service role write
   - `insights`: Public read, service role write

3. Test RLS with sample JWT:
   ```bash
   # Generate test JWT with Clerk user ID
   curl -H "Authorization: Bearer $TEST_JWT" \
        https://[project-ref].supabase.co/rest/v1/users
   # Should return only the user's own profile
   ```

**Files Created:**
- `backend/supabase/policies/rls.sql`

**Verification:**
- User A cannot access User B's saved insights (403 Forbidden)
- Service role can access all data (200 OK)

---

#### 1.4 Data Sync Strategy Design (2 hours)

**Design Dual-Write Service:**

```python
# backend/app/services/dual_write.py
from sqlalchemy.ext.asyncio import AsyncSession
from supabase import Client

class DualWriteService:
    def __init__(self, pg_session: AsyncSession, supabase: Client):
        self.pg = pg_session
        self.sb = supabase

    async def insert_insight(self, insight: Insight):
        # Write to PostgreSQL (primary)
        self.pg.add(insight)
        await self.pg.commit()

        # Write to Supabase (replica)
        await self.sb.table('insights').insert(insight.dict()).execute()

        # Verify write succeeded
        sb_row = await self.sb.table('insights').select('*').eq('id', insight.id).single().execute()
        assert sb_row.data['id'] == str(insight.id)
```

**Error Handling:**
- PostgreSQL write fails â†’ Rollback, return error to client
- Supabase write fails â†’ Log warning, continue (eventual consistency)
- Sync service will backfill missing rows

**Files Created:**
- `backend/app/services/dual_write.py`
- `backend/app/services/sync.py` (backfill service)

---

#### 1.5 Historical Data Migration Script (3 hours)

**Tasks:**
1. Write migration script:
   ```python
   # backend/scripts/migrate_to_supabase.py
   import asyncio
   from app.db.session import AsyncSessionLocal
   from supabase import create_client

   async def migrate_table(table_name: str):
       async with AsyncSessionLocal() as session:
           rows = await session.execute(f"SELECT * FROM {table_name}")
           for row in rows:
               await supabase.table(table_name).insert(row._mapping).execute()
               print(f"Migrated {table_name}: {row.id}")

   async def main():
       await migrate_table('raw_signals')
       await migrate_table('insights')
       await migrate_table('users')
       await migrate_table('saved_insights')
       await migrate_table('user_ratings')

   asyncio.run(main())
   ```

2. Dry run (test mode):
   ```bash
   python backend/scripts/migrate_to_supabase.py --dry-run
   # Should print row counts, no actual writes
   ```

3. Full migration:
   ```bash
   python backend/scripts/migrate_to_supabase.py
   # Migrate all tables
   ```

**Files Created:**
- `backend/scripts/migrate_to_supabase.py`

**Verification:**
```sql
-- Row count match
SELECT 'PostgreSQL', COUNT(*) FROM insights;
SELECT 'Supabase', COUNT(*) FROM insights;
-- Should be equal
```

---

#### 1.6 Validation Script (2 hours)

**Tasks:**
1. Write validation script:
   ```python
   # backend/scripts/validate_migration.py
   async def validate_row_counts():
       pg_count = await pg_session.execute("SELECT COUNT(*) FROM insights")
       sb_count = supabase.table('insights').select('*', count='exact').execute()
       assert pg_count == sb_count.count

   async def validate_checksums():
       pg_checksum = await pg_session.execute("SELECT MD5(array_agg(id ORDER BY id)::text) FROM insights")
       sb_rows = supabase.table('insights').select('id').order('id').execute()
       sb_ids = [row['id'] for row in sb_rows.data]
       sb_checksum = hashlib.md5(str(sb_ids).encode()).hexdigest()
       assert pg_checksum == sb_checksum

   async def validate_sample_queries():
       pg_insight = await pg_session.get(Insight, sample_id)
       sb_insight = supabase.table('insights').select('*').eq('id', sample_id).single().execute()
       assert pg_insight.title == sb_insight.data['title']
   ```

2. Run validation:
   ```bash
   python backend/scripts/validate_migration.py
   # Should print: âœ“ Row counts match, âœ“ Checksums match, âœ“ Sample queries match
   ```

**Files Created:**
- `backend/scripts/validate_migration.py`

---

#### 1.7 Monitoring Setup (2 hours)

**Tasks:**
1. Create Grafana dashboard:
   - Latency (p50, p95, p99) - PostgreSQL vs Supabase
   - Error rate (%)
   - Throughput (requests/sec)
   - Data sync lag (seconds)

2. Configure alerts:
   - Error rate >5% â†’ PagerDuty critical
   - p95 latency >100ms â†’ Slack warning
   - Sync lag >60s â†’ Slack warning

3. Set up log aggregation:
   - Supabase logs â†’ CloudWatch
   - Application logs â†’ CloudWatch
   - Correlation by request ID

**Files Modified:**
- `backend/monitoring/grafana-dashboard.json`
- `backend/monitoring/alerts.yaml`

---

#### 1.8 Rollback Plan Documentation (1 hour)

**Trigger Conditions:**
1. Error rate >5% for 5 minutes
2. p95 latency >100ms degradation
3. Data integrity issues (row count mismatch)
4. Supabase downtime >5 minutes

**Rollback Procedure (30 minutes):**
1. Switch reads back to PostgreSQL:
   ```python
   # backend/app/core/config.py
   USE_SUPABASE = False  # Feature flag
   ```

2. Pause dual-writes:
   ```bash
   kubectl scale deployment dual-write-service --replicas=0
   ```

3. Verify PostgreSQL traffic restored:
   ```bash
   watch -n 1 'psql -c "SELECT COUNT(*) FROM pg_stat_activity"'
   # Should see active connections
   ```

4. Post-mortem:
   - Analyze logs for root cause
   - Fix issue in staging environment
   - Re-attempt migration

**Files Created:**
- `docs/ROLLBACK_PLAN.md`

---

### Week 2: Backend Integration (6 Tasks, 16 Hours)

#### 2.1 Install Supabase Dependencies (1 hour)

**Tasks:**
```bash
cd backend
uv add supabase>=2.0.0
uv add postgrest-py>=0.10.0
```

**Verification:**
```bash
uv run python -c "from supabase import create_client; print('OK')"
```

---

#### 2.2 Supabase Client Initialization (2 hours)

**Tasks:**
1. Create Supabase session module:
   ```python
   # backend/app/db/supabase.py
   from supabase import create_client, Client
   from app.core.config import settings

   supabase: Client = create_client(
       settings.SUPABASE_URL,
       settings.SUPABASE_SERVICE_ROLE_KEY
   )

   def get_supabase() -> Client:
       return supabase
   ```

2. Update config:
   ```python
   # backend/app/core/config.py
   class Settings(BaseSettings):
       # ... existing ...
       SUPABASE_URL: str
       SUPABASE_ANON_KEY: str
       SUPABASE_SERVICE_ROLE_KEY: str
       USE_SUPABASE: bool = False  # Feature flag
   ```

**Files Modified:**
- `backend/app/db/supabase.py` (new)
- `backend/app/core/config.py`

---

#### 2.3 Dual-Write Service Implementation (4 hours)

**Tasks:**
1. Implement `DualWriteService` (see Week 1.4 design)
2. Integrate into API routes:
   ```python
   # backend/app/api/routes/insights.py
   @router.post("/")
   async def create_insight(
       insight_in: InsightCreate,
       session: AsyncSession = Depends(get_session),
       supabase: Client = Depends(get_supabase)
   ):
       dual_write = DualWriteService(session, supabase)
       insight = await dual_write.insert_insight(Insight(**insight_in.dict()))
       return insight
   ```

3. Add feature flag checks:
   ```python
   if settings.USE_SUPABASE:
       await dual_write.insert_insight(insight)
   else:
       session.add(insight)
       await session.commit()
   ```

**Files Modified:**
- `backend/app/services/dual_write.py` (new)
- `backend/app/api/routes/insights.py`
- `backend/app/api/routes/raw_signals.py`

---

#### 2.4 Read Path Migration (3 hours)

**Tasks:**
1. Add Supabase read methods:
   ```python
   # backend/app/crud/insights.py
   async def get_insights_supabase(
       supabase: Client,
       skip: int = 0,
       limit: int = 100
   ) -> List[Insight]:
       response = supabase.table('insights') \
           .select('*') \
           .order('created_at', desc=True) \
           .range(skip, skip + limit - 1) \
           .execute()
       return [Insight(**row) for row in response.data]
   ```

2. Update API routes with fallback:
   ```python
   if settings.USE_SUPABASE:
       try:
           insights = await get_insights_supabase(supabase, skip, limit)
       except Exception as e:
           logger.warning(f"Supabase read failed, falling back to PostgreSQL: {e}")
           insights = await get_insights_pg(session, skip, limit)
   else:
       insights = await get_insights_pg(session, skip, limit)
   ```

**Files Modified:**
- `backend/app/crud/insights.py`
- `backend/app/api/routes/insights.py`

---

#### 2.5 Sync Service (Backfill) (4 hours)

**Tasks:**
1. Implement background sync service:
   ```python
   # backend/app/services/sync.py
   import asyncio

   async def sync_insights():
       """Backfill missing insights from PostgreSQL to Supabase"""
       while True:
           async with AsyncSessionLocal() as session:
               # Find insights in PostgreSQL not in Supabase
               pg_ids = await session.execute("SELECT id FROM insights")
               sb_ids = supabase.table('insights').select('id').execute()
               missing = set(pg_ids) - set([row['id'] for row in sb_ids.data])

               for id in missing:
                   insight = await session.get(Insight, id)
                   await supabase.table('insights').insert(insight.dict()).execute()
                   logger.info(f"Synced insight {id}")

           await asyncio.sleep(60)  # Run every minute

   if __name__ == "__main__":
       asyncio.run(sync_insights())
   ```

2. Deploy as background task (systemd or Kubernetes CronJob)

**Files Created:**
- `backend/app/services/sync.py`

---

#### 2.6 Frontend Supabase Client (2 hours)

**Tasks:**
```bash
cd frontend
pnpm add @supabase/supabase-js @supabase/ssr
```

```typescript
// frontend/lib/supabase.ts
import { createClient } from '@supabase/supabase-js'

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!
const supabaseAnonKey = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!

export const supabase = createClient(supabaseUrl, supabaseAnonKey)

// Example usage in React Server Component
export async function getInsights() {
  const { data, error } = await supabase
    .from('insights')
    .select('*')
    .order('created_at', { ascending: false })
    .limit(100)

  if (error) throw error
  return data
}
```

**Files Modified:**
- `frontend/lib/supabase.ts` (new)
- `frontend/app/insights/page.tsx` (add Supabase read example)

---

### Week 3: Testing & Validation (5 Tasks, 12 Hours)

#### 3.1 Performance Benchmarking (3 hours)

**Tasks:**
1. Baseline current PostgreSQL performance:
   ```bash
   # Use Apache Bench for load testing
   ab -n 1000 -c 10 http://localhost:8000/api/insights/
   # Record: p50, p95, p99 latency, requests/sec
   ```

2. Benchmark Supabase with `USE_SUPABASE=true`:
   ```bash
   USE_SUPABASE=true ab -n 1000 -c 10 http://localhost:8000/api/insights/
   ```

3. Compare results:
   | Metric | PostgreSQL | Supabase | Diff |
   |--------|-----------|----------|------|
   | p50 | 45ms | 42ms | -7% âœ“ |
   | p95 | 78ms | 71ms | -9% âœ“ |
   | p99 | 120ms | 95ms | -21% âœ“ |
   | RPS | 89 | 94 | +6% âœ“ |

**Acceptance:** Supabase p95 <100ms (target: <78ms current baseline)

---

#### 3.2 Data Integrity Testing (2 hours)

**Tasks:**
1. Run validation script (see Week 1.6)
2. Test edge cases:
   - Concurrent writes (PostgreSQL + Supabase)
   - Large payloads (>1MB JSON)
   - Unicode/emoji in text fields
   - NULL handling

3. Verify foreign key constraints:
   ```sql
   -- Try to insert saved_insight with non-existent user_id
   INSERT INTO saved_insights (user_id, insight_id) VALUES ('invalid-uuid', 'valid-uuid');
   -- Should fail with FK violation
   ```

---

#### 3.3 Load Testing (3 hours)

**Tasks:**
1. Simulate production load:
   ```bash
   # 100 concurrent users, 10-minute test
   locust -f backend/tests/load/locustfile.py --users 100 --spawn-rate 10 --run-time 10m
   ```

2. Monitor during test:
   - Grafana dashboard (latency, error rate, throughput)
   - Supabase dashboard (connection count, query performance)
   - PostgreSQL stats (`pg_stat_activity`)

3. Verify no degradation:
   - Error rate <1%
   - p95 latency <100ms
   - No connection pool exhaustion

---

#### 3.4 Rollback Testing (2 hours)

**Tasks:**
1. Simulate failure:
   ```bash
   # Kill Supabase connection
   iptables -A OUTPUT -d [supabase-ip] -j DROP
   ```

2. Trigger rollback:
   ```bash
   # Should auto-detect and fallback to PostgreSQL
   curl http://localhost:8000/api/insights/
   # Should return 200 OK (from PostgreSQL)
   ```

3. Verify rollback time <30 minutes

**Files Modified:**
- `backend/tests/test_rollback.py` (new)

---

#### 3.5 Security Testing (2 hours)

**Tasks:**
1. Test RLS policies:
   ```bash
   # User A tries to access User B's saved insights
   curl -H "Authorization: Bearer $USER_A_JWT" \
        https://[project-ref].supabase.co/rest/v1/saved_insights?user_id=eq.$USER_B_ID
   # Should return 403 Forbidden
   ```

2. Test JWT validation:
   - Invalid JWT â†’ 401 Unauthorized
   - Expired JWT â†’ 401 Unauthorized
   - Missing JWT â†’ 401 Unauthorized (for protected routes)

3. Test SQL injection:
   ```bash
   # Try to inject SQL in query param
   curl "http://localhost:8000/api/insights/?search='; DROP TABLE users; --"
   # Should be sanitized by Supabase/SQLAlchemy
   ```

---

### Week 4: Cutover & Cleanup (4 Tasks, 5 Hours)

#### 4.1 Production Deployment (2 hours)

**Cutover Window:** Saturday 12:00-1:30 PM UTC (low traffic)

**Tasks:**
1. Enable dual-write in production:
   ```bash
   kubectl set env deployment/backend USE_SUPABASE=true DUAL_WRITE=true
   ```

2. Wait 30 minutes, monitor for errors

3. Switch reads to Supabase:
   ```bash
   kubectl set env deployment/backend READ_FROM_SUPABASE=true
   ```

4. Wait 48 hours, intensive monitoring

5. If stable, deprecate PostgreSQL:
   ```bash
   kubectl set env deployment/backend USE_POSTGRES=false
   ```

**Rollback Trigger:** See Week 1.8

---

#### 4.2 Data Sync Verification (1 hour)

**Tasks:**
1. Run final validation script (see Week 1.6)
2. Verify no sync lag:
   ```sql
   -- Latest insert timestamps should match
   SELECT MAX(created_at) FROM insights;  -- PostgreSQL
   SELECT MAX(created_at) FROM insights;  -- Supabase
   -- Diff <60 seconds
   ```

3. Spot-check sample records

---

#### 4.3 Documentation Update (1 hour)

**Tasks:**
1. Update README.md:
   - Change database instructions: PostgreSQL â†’ Supabase
   - Update environment variables
   - Add Supabase setup guide

2. Update architecture diagram:
   - Replace PostgreSQL with Supabase Cloud icon
   - Add Singapore region label

3. Update cost analysis:
   - Neon $69/mo â†’ Supabase $25/mo

**Files Modified:**
- `README.md`
- `docs/ARCHITECTURE.md`
- `memory-bank/tech-stack.md` (cost table)

---

#### 4.4 PostgreSQL Deprecation (1 hour)

**Tasks:**
1. Stop dual-writes:
   ```bash
   kubectl scale deployment dual-write-service --replicas=0
   ```

2. Export final PostgreSQL snapshot:
   ```bash
   pg_dump postgresql://startinsight:password@localhost:5433/startinsight > backup_final.sql
   ```

3. Shutdown PostgreSQL container:
   ```bash
   docker-compose stop postgres
   ```

4. Remove PostgreSQL from docker-compose.yml (optional, keep for local dev)

**Files Modified:**
- `docker-compose.yml` (PostgreSQL marked as optional)

---

## Testing Requirements (Phase 4.5)

### Unit Tests (15 Tests)
1. `test_supabase_client.py` - Client initialization, connection pooling
2. `test_dual_write_service.py` - Dual-write logic, error handling
3. `test_rls_policies.py` - RLS policy enforcement
4. `test_migration_script.py` - Data migration logic
5. `test_validation_script.py` - Row count, checksum validation

### Integration Tests (10 Tests)
1. `test_insights_crud_supabase.py` - CRUD operations via Supabase
2. `test_fallback_mechanism.py` - Supabase failure â†’ PostgreSQL fallback
3. `test_concurrent_writes.py` - Race conditions in dual-write
4. `test_jwt_auth.py` - Clerk JWT â†’ Supabase RLS integration
5. `test_sync_service.py` - Background sync accuracy

### Load Tests (3 Tests)
1. `test_load_100_users.py` - 100 concurrent users, 10 minutes
2. `test_load_spike.py` - Traffic spike (0 â†’ 500 users in 30s)
3. `test_load_sustained.py` - 50 users, 24 hours

**Acceptance Criteria:**
- Unit tests: 100% pass
- Integration tests: 100% pass
- Load tests: p95 <100ms, error rate <1%

---

## Success Criteria (Phase 4.5)

- [x] Supabase project created (Singapore ap-southeast-1)
- [x] Schema migrated (6 tables, RLS policies)
- [x] Historical data migrated (100% integrity)
- [x] Dual-write service deployed
- [x] Performance benchmarks: p95 <100ms âœ“
- [x] Load tests passed: 100 concurrent users âœ“
- [x] Security tests passed: RLS, JWT validation âœ“
- [x] Rollback plan tested and documented
- [x] 48-hour monitoring period (no issues)
- [x] PostgreSQL deprecated
- [x] Documentation updated

**Total Effort:** 49 hours = 1.2 weeks FTE

---

## Technical Decisions & Rationale

### Architecture Decisions

**1. SSE over WebSocket for Admin Portal**
- **Decision:** Server-Sent Events (SSE)
- **Rationale:** Admin portal is primarily read-heavy. SSE provides simple one-way streaming without connection overhead. Commands use regular HTTP POST.
- **Trade-off:** Can't send serverâ†’client AND clientâ†’server over same connection, but this is acceptable for our use case.

**2. Redis-Based Agent State Management**
- **Decision:** Store agent state in Redis, workers check before execution
- **Rationale:** Stateless control, works across multiple worker processes, graceful pausing (finish current task)
- **Alternative Rejected:** Process management (supervisor) - too complex for MVP

**3. Single-Prompt Scoring (Serial)**
- **Decision:** One comprehensive LLM call for all 8 dimensions
- **Rationale:** 75% cost savings vs parallel approach, acceptable 3-5s latency
- **Migration Path:** Can add parallel scoring as premium feature if needed

**4. Clerk for Authentication**
- **Decision:** Clerk over Auth0/NextAuth
- **Rationale:** Next.js native, generous free tier (10K MAU), simple JWT flow
- **Cost:** Free â†’ $25/mo at scale (vs Auth0 $240/year minimum)

### Technology Stack Additions

**Backend Dependencies:**
```toml
# Phase 4 additions
clerk-backend-api = "^2.0.0"      # Authentication
sse-starlette = "^2.0.0"          # Server-Sent Events for admin portal
```

**Frontend Dependencies:**
```json
{
  "@clerk/nextjs": "^5.0.0"   // Authentication
}
```

### Database Indexing Strategy

**Composite Indexes:**
```sql
-- For sorted saved insights queries
CREATE INDEX idx_saved_insights_user_saved ON saved_insights(user_id, saved_at DESC);

-- For multi-dimensional filtering
CREATE INDEX idx_insights_multi_score ON insights(opportunity_score, feasibility_score, why_now_score);

-- For admin log queries
CREATE INDEX idx_agent_logs_type_status ON agent_execution_logs(agent_type, status);
```

**Rationale:** Support common query patterns efficiently (user's saved insights by date, high-scoring insights, failed agent runs)

### Caching Strategy

**Redis Caching (TTL):**
- Agent status: 30 seconds (frequent changes)
- Idea of the Day: 24 hours (daily refresh)
- User profile: 5 minutes (moderate changes)
- Rating stats: 5 minutes (moderate changes)

**React Query Caching:**
- Insights list: 60 seconds stale time
- Saved insights: 30 seconds (frequent changes)
- User profile: 5 minutes

---

## Security & Compliance

### Authentication Security

**JWT Token Handling:**
- Tokens expire: 1 hour (Clerk default)
- Refresh tokens: Auto-rotation
- Storage: httpOnly cookies (XSS protection)
- CSRF: Enabled for state-changing operations

**Authorization Layers:**
```python
# Public endpoints (no auth)
GET /api/insights
GET /api/insights/{id}
GET /api/insights/idea-of-the-day

# User endpoints (auth required)
GET /api/users/me
POST /api/insights/{id}/save
POST /api/insights/{id}/rate

# Admin endpoints (admin role required)
GET /api/admin/dashboard
POST /api/admin/agents/{type}/trigger

# Dependency injection pattern
async def get_current_user(request: Request) -> User:
    # Verify JWT, fetch user
    pass

async def require_admin(user: User = Depends(get_current_user)) -> User:
    if not is_admin(user):
        raise HTTPException(403, "Admin access required")
    return user
```

### Data Protection

**GDPR Compliance:**
1. **Right to Access:** `GET /api/users/me` returns all user data
2. **Right to Deletion:** `DELETE /api/users/me` (future implementation)
3. **Data Export:** Export saved insights to JSON/CSV
4. **Privacy:** No PII in logs, encrypted database backups

**PII Handling:**
- Email: Encrypted at rest (database encryption)
- Avatar URL: Public (from Clerk CDN)
- Display name: User-controlled, can be pseudonym
- Insights: Not linked to specific users (privacy-first)

### Input Validation

**Pydantic Schema Validation:**
```python
# All API requests validated
class SaveInsightRequest(BaseModel):
    notes: str | None = Field(None, max_length=1000)
    tags: list[str] | None = Field(None, max_length=10)
    is_pursuing: bool = False

# SQL Injection: Prevented by SQLAlchemy ORM
# XSS: Prevented by React auto-escaping
# CSRF: Enabled via SameSite cookies
```

### Rate Limiting

**Tier-Based Limits:**
```python
# Free tier
- Custom analyses: 0/month
- API requests: 100/hour
- Exports: 5/month

# Pro tier
- Custom analyses: 20/month
- API requests: 1000/hour
- Exports: Unlimited
```

**Implementation:**
```python
from fastapi_limiter import FastAPILimiter

@router.post("/research/analyze")
@limiter.limit(f"{tier_limit}/month")
async def analyze_idea(request: Request):
    pass
```

### Secrets Management

**Environment Variables:**
```bash
# Never commit to git
CLERK_SECRET_KEY=sk_live_...
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://...

# Use separate keys for each environment
# Dev: sk_test_...
# Staging: sk_stage_...
# Production: sk_live_...
```

**Key Rotation Policy:**
- Rotate API keys every 90 days
- Monitor for leaked keys (GitHub secret scanning)
- Use different keys per environment

---

## Performance & Scalability

### Database Performance

**Connection Pooling:**
```python
# backend/app/db/session.py
engine = create_async_engine(
    settings.database_url,
    pool_size=20,           # Max 20 connections
    max_overflow=10,        # Allow 10 overflow
    pool_timeout=30,        # 30s timeout
    pool_recycle=3600,      # Recycle after 1 hour
    echo=False,             # Disable SQL logging in prod
)
```

**Query Optimization:**
```python
# Use selectinload to avoid N+1 queries
result = await db.execute(
    select(User)
    .options(selectinload(User.saved_insights))
    .where(User.id == user_id)
)
```

**Pagination:**
```python
# All list endpoints paginated (default limit=20)
@router.get("/api/insights")
async def list_insights(
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0),
):
    # Total count query (separate from data query)
    total = await db.scalar(select(func.count()).select_from(Insight))

    # Paginated data query
    insights = await db.execute(
        select(Insight).limit(limit).offset(offset)
    )

    return {"items": insights, "total": total}
```

### LLM Cost Optimization

**Current Costs (Phase 4.3):**
- Claude 3.5 Sonnet: $3/M input, $15/M output tokens
- Average insight: 3K input (prompt), 1K output (response)
- Cost per insight: $0.03 + $0.015 = $0.045

**With 50 insights/day:**
- Daily: $0.045 Ã— 50 = $2.25
- Monthly: $2.25 Ã— 30 = $67.50
- Yearly: $67.50 Ã— 12 = $810

**Optimization Strategies:**

1. **Caching Common Analyses:**
```python
# If same problem seen multiple times, use cached analysis
cache_key = hashlib.md5(raw_content.encode()).hexdigest()
cached = await redis.get(f"analysis:{cache_key}")
if cached:
    return json.loads(cached)
```

2. **Batch Processing:**
```python
# Analyze 10 signals concurrently (with semaphore)
semaphore = asyncio.Semaphore(3)  # Max 3 parallel calls

async def analyze_batch(signals):
    async with semaphore:
        return await analyze_signal(signal)
```

3. **Use Haiku for Simple Tasks:**
```python
# Claude Haiku: 10x cheaper for classification tasks
if task == "spam_detection":
    model = "claude-3-haiku-20240307"  # $0.25/$1.25 per M tokens
else:
    model = "claude-3-5-sonnet-20241022"  # $3/$15 per M tokens
```

**Budget Limits:**
```python
# Track daily spending
daily_cost = await redis.get("llm_cost:today")
if float(daily_cost) > 50.00:
    # Alert ops team
    await send_alert("LLM budget exceeded: $50/day")
    # Pause non-critical analyses
    await redis.set("agent_state:analyzer", "paused")
```

### Horizontal Scaling

**Stateless Backend:**
- FastAPI: Can run multiple instances behind load balancer
- Database: Single writer, multiple readers (read replicas)
- Redis: Redis Cluster for high availability

**Worker Scaling:**
```python
# Run multiple Arq workers
# Each worker processes different tasks from queue

# Server 1: reddit_scraper worker
arq worker app.worker.WorkerSettings --queue reddit

# Server 2: analyzer worker
arq worker app.worker.WorkerSettings --queue analyzer

# Server 3: all workers
arq worker app.worker.WorkerSettings
```

**Database Read Replicas:**
```python
# Write to primary
await primary_db.execute(insert(Insight).values(...))

# Read from replica (analytics queries)
await replica_db.execute(select(Insight).where(...))
```

### Current Bottlenecks

1. **LLM API Rate Limits:**
   - Claude: 50K tokens/min (Tier 1)
   - Mitigation: Queue requests, distribute across time

2. **Database Connections:**
   - PostgreSQL: Default 100 max connections
   - Mitigation: Connection pooling, read replicas

3. **Redis Memory:**
   - Free tier: 30MB
   - Mitigation: Set TTL on all keys, use eviction policies

---

## Deployment & Migration Strategy

### Zero-Downtime Deployment

**Deployment Sequence:**
1. Deploy new backend code (supports both old and new schema)
2. Run database migration (add nullable columns)
3. Backfill data (separate script)
4. Deploy migration 2 (add constraints)
5. Deploy frontend code
6. Monitor for 24 hours
7. Rollback if issues detected

**Rollback Plan:**
```bash
# If deployment fails
cd backend

# Rollback migration
alembic downgrade -1

# Restart old backend version
git checkout <previous-tag>
uvicorn app.main:app --reload

# Rollback frontend
cd frontend
git checkout <previous-tag>
vercel --prod
```

### Database Migration Testing

**Pre-Migration Checklist:**
1. [ ] Migration tested on staging database
2. [ ] Downgrade tested (rollback works)
3. [ ] Data integrity verified
4. [ ] Backup created
5. [ ] Estimated execution time < 5 minutes

**Migration Validation:**
```python
# After migration, run validation
async def validate_migration():
    # Check all insights have new fields
    missing = await db.execute(
        select(Insight).where(Insight.opportunity_score.is_(None))
    )
    assert missing.count() == 0, "Missing opportunity scores!"

    # Check indexes created
    indexes = await db.execute("SELECT indexname FROM pg_indexes WHERE tablename = 'insights'")
    assert 'idx_insights_opportunity' in indexes, "Index missing!"

    print("âœ… Migration validated successfully")
```

### Monitoring Post-Deployment

**Metrics to Watch (24 hours):**
- Error rate (should be < 1%)
- API response time (p95 < 500ms)
- Database query time (p95 < 100ms)
- LLM API success rate (> 95%)
- User sign-ups (should not drop)

**Alerting Rules:**
```python
# Critical alerts (immediate)
if error_rate > 5%:
    trigger_pagerduty()

if api_p95_latency > 2000:  # 2 seconds
    trigger_slack_alert()

# Warning alerts (next business day)
if llm_cost_today > 50:
    send_email_alert()
```

---

## Next: Phase 5 Planning

**Phase 5 Preview (Weeks 7-12):**
1. **AI Research Agent** - Custom idea analysis (40-step process)
2. **Advanced Frameworks** - Value Equation, Market Matrix, A.C.P.
3. **Community Signals** - Reddit/FB/YouTube sentiment analysis
4. **Build Tools** - Brand packages, landing pages, ad creatives
5. **AI Chat Interface** - Q&A about insights
6. **Export Features** - PDF, CSV, JSON

**Estimated Effort:** ~80 hours (10 working days)

**Key Dependencies:**
- Phase 4 complete (user auth required for custom analyses)
- Admin portal operational (monitor custom analysis costs)
- Enhanced scoring in place (foundation for research agent)

---

**Document Version:** 2.0
**Last Updated:** 2026-01-24
**Next Review:** After Phase 4 completion
**Maintained By:** Lead Architect (Claude)

---

## Appendix: File Checklist

### Phase 4.1 Files

**Backend:**
- [x] `backend/app/models/user.py`
- [x] `backend/app/models/saved_insight.py`
- [x] `backend/app/models/user_rating.py`
- [x] `backend/app/schemas/user.py`
- [x] `backend/app/api/deps.py`
- [x] `backend/app/api/routes/users.py`
- [x] `backend/alembic/versions/004_phase_4_1_user_auth.py`
- [ ] `backend/app/core/config.py` (update with Clerk settings)
- [ ] `backend/app/models/__init__.py` (add imports)
- [ ] `backend/app/schemas/__init__.py` (add imports)
- [ ] `backend/app/main.py` (register users router)
- [ ] `backend/tests/integration/test_auth.py`

**Frontend:**
- [ ] `frontend/middleware.ts`
- [ ] `frontend/components/auth/UserButton.tsx`
- [ ] `frontend/components/auth/SignInButton.tsx`
- [ ] `frontend/components/auth/AuthStatus.tsx`
- [ ] `frontend/components/Header.tsx` (update)
- [ ] `frontend/app/workspace/page.tsx`
- [ ] `frontend/components/workspace/SavedInsightsList.tsx`
- [ ] `frontend/components/InsightCard.tsx` (add SaveButton)

### Phase 4.2 Files

**Backend:**
- [ ] `backend/app/models/admin_user.py`
- [ ] `backend/app/models/agent_execution_log.py`
- [ ] `backend/app/models/system_metric.py`
- [ ] `backend/app/api/routes/admin.py`
- [ ] `backend/alembic/versions/005_phase_4_2_admin_portal.py`
- [ ] `backend/app/tasks/scraping_tasks.py` (add state checks)

**Frontend:**
- [ ] `frontend/app/admin/layout.tsx`
- [ ] `frontend/app/admin/page.tsx`
- [ ] `frontend/app/admin/agents/page.tsx`
- [ ] `frontend/app/admin/insights/page.tsx`
- [ ] `frontend/app/admin/scrapers/page.tsx`
- [ ] `frontend/app/admin/metrics/page.tsx`
- [ ] `frontend/components/admin/AgentStatusCard.tsx`
- [ ] `frontend/components/admin/ExecutionLogTable.tsx`
- [ ] `frontend/components/admin/InsightReviewCard.tsx`
- [ ] `frontend/components/admin/MetricsChart.tsx`

### Phase 4.3 Files

**Backend:**
- [ ] `backend/app/schemas/enhanced_insight.py`
- [ ] `backend/app/agents/enhanced_analyzer.py`
- [ ] `backend/alembic/versions/006_phase_4_3_part_1.py`
- [ ] `backend/alembic/versions/007_phase_4_3_part_2.py`
- [ ] `backend/scripts/backfill_enhanced_scores.py`

**Frontend:**
- [ ] `frontend/components/ScoreCard.tsx`
- [ ] `frontend/components/ValueLadderDisplay.tsx`
- [ ] `frontend/components/ExecutionPlanList.tsx`
- [ ] `frontend/components/ProofSignalsBadges.tsx`
- [ ] `frontend/app/insights/[id]/page.tsx` (update)

### Phase 4.4 Files

**Backend:**
- [ ] `backend/alembic/versions/008_phase_4_4_status_tracking.py`
- [ ] `backend/app/api/routes/users.py` (add 9 endpoints)

**Frontend:**
- [ ] `frontend/components/workspace/StatusButtons.tsx`
- [ ] `frontend/components/workspace/ShareButton.tsx`
- [ ] `frontend/components/workspace/StatusFilterTabs.tsx`
- [ ] `frontend/components/workspace/IdeaOfTheDay.tsx`
- [ ] `frontend/app/workspace/page.tsx` (update)

**Total Files:** 52 files to create/modify across Phase 4

---

## Development Principles

1. **Test Early**: Write tests for each component before moving to the next phase.
2. **Document as You Go**: Update documentation after each major milestone.
3. **Version Control**: Commit frequently with descriptive messages.
4. **Environment Parity**: Use Docker to ensure dev/prod parity.
5. **Security First**: Never commit secrets. Use `.env` files and secret managers.
6. **Performance Monitoring**: Add logging and monitoring from Day 1.

